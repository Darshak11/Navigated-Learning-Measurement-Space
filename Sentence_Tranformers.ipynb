{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w04nWmrLFk3B",
        "outputId": "f97e1993-6047-44dc-9743-4576c7d20218"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.64.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.1.97)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.21.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.9.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.6.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence-transformers) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers) (1.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentence-transformers\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline, set_seed\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentenceTransformer('stsb-roberta-large')"
      ],
      "metadata": {
        "id": "7cZ4cS_XGC7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1=open(\"/sample(1).txt\",\"r\")\n",
        "d1=f1.read()\n",
        "f1.close()\n",
        "d1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "ItakB56FGeCV",
        "outputId": "e9d58888-52a2-4d2b-90a4-64fc1b46b231"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Validating The Myth of Average through Evidences\\nMost formal educational practices based on the classroom, imparts skills and knowledge at scale, by adopting the model of a factory. Here, the focus is on creation of formal processes, mass production of measurable outcomes, and standardization. Curricula and educational practices are designed for a hypothetical ``average\" student, having ``average\" abilities. However, recent advances in individualization have found vast discrepancies between  individual traits and group averages. Designing models based on group averages, are usually ineffective when the individual is the target beneficiary. This research proposes Evidence Based Competency Model as a mechanism for providing individualized learning experiences. Here, the term evidence, refers to informal data generated by the learner, pertinent to the learning process. Individual models are built for each learner, based on their learning activities. These models are clustered to observe  similarity in learning patterns among the individual learners. This study also shows significant variability from the ``average model\", validating The Myth of Average. \\nKeywords:\\nEvidence, Average Model, Individualization, Evidence Based Competency Model, Learning Process.\\nIntroduction:\\n\\nThe objective of education is to create empowered individuals that are capable of problem solving, upholding their individuality, and possessing an ability to acquire relevant competencies in their area of interests. Standardization of learning practices based on the classroom, has lead to uniform teaching and assessment practices without regard to how disparate individuals behave, learn, develop and apply their knowledge. \\n\\nThe classroom model is able to provide pedagogical solutions at scale by addressing the needs of a hypothetical ``average\\'\\' individual. However, research on individualization have shown that, as the number of dimensions of concern increase, the probability of finding an individual who is average on all dimensions rapidly diminishes to zero. This is popularly called The Myth of the Average.  A uniform model based on statistical averages does not capture the patterns of variability among individual learners.  Each individual has different interests, disposition, contexts and styles to learn different topics.\\n\\nOne of the emerging approaches towards standardizing pedagogy models, is to focus on learning outcomes. The Outcome Based Education (OBE) model focuses on learner\\'s ability to produce specific, measurable outcomes as part of the learning process. The emphasis on visible outcomes, discounts the holistic nature of education comprising of a number of tacit elements and OBE is also considered to be strongly rooted in behaviorist learning practices, that are incompatible with other learning cultures like constructivist  education.\\n\\nIn this work, we show that data generated during the learning process, referred to as evidence, can be used to reason about the underlying competency. We also show that one single average model to predict the state of the competency cannot be used and we need to build separate models for learners, hence validating the Myth of Average, discussed in more detail,  in.\\n\\nThe use of activity data, rather than assessments and outcomes, for determining learner\\'s latent competency levels, have been addressed for specific activities in. The focus here has been to correlate different types of learning activities like watching video or learning by doing activities, to their implication on learning. \\n\\nIn this work, we don\\'t focus on any specific learning activity and follow a generic approach to collect any kind of learning data and use machine learning techniques to correlate characteristics of activity data with data from outcomes and formal assessments. We focus on activities involving learners consuming resources like videos, text books, articles, documents in the current implementation of the model.\\n\\nEvidence Modeling:\\nThe term ``evidence\\'\\' is contrasted with ``outcomes\\'\\' as follows: outcomes refer to assessment data collected from formal testing environments, where the learner is fully cognizant of being assessed and has explicitly prepared for the same. In contrast, evidence pertains to data collected on an implicit, continuous basis on any activity of the learner pertaining to the competency in question.\\nExisting methods of determining the underlying competency of a learner through formal assessments and visible outcomes, has its own issues. \\nThere is a need for models that uses evidences, based on the learner\\'s activities and maps these learners to their competencies.\\n\\nWe propose a model that explores the activity data of the learners, generated while achieving the competency. The data, collected implicitly in a continuous fashion, is modeled to map the learners to their competencies.\\n\\nEvidences can be considered analogous to an observation that an individual makes, in the classroom through interaction or silent observation of reactions, which contains significant insights. An offline system like a classroom is unable to gather data for each individual during the learning process, but when learning happens in a Technology Assisted Learning Environment(TALE), data is continuously captured by the platform. Technology augmentation can happen in various ways -- like sensors and RFID tags to record attendance, recording and analysis of students\\' classroom activities like questions, discussions, etc. We want to focus on the evidences, so that we dont have to rely only on the formal assessments to determine the competency of the learner. \\n\\nA competency model based on evidences can also be used to identify anomalous cases where the outcomes state that the individual has the competency while the evidence indicates a lack of competency, or vice-versa. A teacher can only analyze such cases and address them appropriately. The model based on evidences, acts as a way of aiding the teacher in teaching and evaluating process. % It can also help the students identify areas they are weak in, and help them acquire the competency.\\n\\nNewer learning domains like training drivers to learn a particular language that would improve their communication skills, may not have standardized competency models to aid in pedagogy and assessments. In such cases, models based on evidences would help to map the learners to their competencies. \\n\\nThe main challenge is to create an evidence model for collecting data, and to argue for the completeness of the evidence. Learning may happen outside of the evidence gathering, and different kinds of learning activities may require different kinds of evidences to reason about them. \\n\\nIn order to address the above challenge, we adopt a least-biased model for evidence modeling, and treat each form of evidence as equally important in the input feature vector. All forms of evidences collected are then given as input to a machine learning algorithm to find the best possible indicators for the outcomes based on assessment scores. \\n\\nExperiments and Initial Results:\\n\\nThe activity data used to build models is collected from a large, open online learning platform, implemented across several schools in the US. The platform has aggregated open learning resources for various courses, provided by content creators, curators and instructors. The learning resource can be a document, video, audio, puzzles or any content used to obtain the competency. Learners enroll to various courses. \\n\\nA course is organized into several competencies, where a competency is seen as the basic unit of learning. Each competency may have several learning resources mapped to it. Students consume learning resources and whenever they are ready, give assessments to earn a score. Instructors evaluate the assessments and provide their feedback in the form of scores. The scores indicate the status of the learner with respect to the competency. The learning resources are mapped to competencies for various courses like Maths, Science, English and Social Science in the K-12 curriculum.  %Each competency also has a signature assessment that the learner has to take in order to earn a status for that competency. \\n\\nThe activity data collected for a (learner, competency) pair, is divided into collections and assessments. Collections are resources used during the learning process, while assessments act as indicators of learning. In the platform, a learner is said to have achieved a ``completed\" status for a competency if one gets more than 80% in the respective assessments. The platform does not award a ``fail\" status to the learner. The learner keeps attempting assessments multiple times until the learner gets 80% or more. The status is then set to ``completed\", else the status is marked as ``in-progress\". %The data used to build Evidence based competency model contains learners who have completed status as well as in-progress status with respect to a particular competency.\\n\\nWhenever a learner consumes a resource to learn a concept, an event is logged in the system with the details of the resource, time of the event, learner details and type of event(started consuming the resource). The same process is repeated when the learner finishes consuming the resource with a stop event. The events are captured for all the courses. Individuals consume various resources mapped to the same competency and at the end give assessments to get a particular status for that competency.  \\n\\nUsing these activity data we have built a model to determine the outcome of the competency based on the evidences.\\n\\nExperiment 1: \\nWe built a Support Vector Machine (SVM) classifier to test a hypothesis. Our hypothesis is as follows: Ht: The time spent on learning resources, the total number of resources consumed by learners, can predict the outcome for the underlying competency.\\n\\nThe features identified are total time spent on resources, average time spent on resources and number of resources used for acquiring the corresponding competency. The users were given a completed or in-progress status based on their assessment scores. That serves as the ground truth for our model. \\n\\nThe dataset has 28000 (user,competency) pairs with their corresponding evidences. This data was divided into 80-20 split randomly for training and testing the SVM model respectively. We built a single SVM model for all learners and their competencies in all courses. We classified the data using linear, polynomial, sigmoid and radial basis kernel function. The data was not linearly separable. The radial basis kernel function was found to be the best kernel function to classify the data in terms of Accuracy, Precision and F1 score as shown in Table 1. \\n\\nKernel functions Accuracy Precision F1 score\\nRadial basis 82.43% 68.79% 51.13%\\nLinear 78.39% 53.65% 40.30%\\nPolynomial 78.83% 56.25% 37.91%\\nSigmoid 68.84% 30.68% 30.37%\\n\\nTable 1: Performance metrics comparing the kernels\\n\\nThe accuracy measure of the SVM model using the radial basis function states that using total time spent, average time spent on the resources and number of resources, we can significantly predict the outcome of the underlying competency. The outcomes are determined by the scores of the assessments, which was not used as feature to build the models. This shows that evidence can be used as an alternate way to model the outcome of the underlying competency. \\n\\nThe same model was made to classify all the competencies of a random individual learner, the accuracy of the average SVM model varied between 30% to 100% for different learners . The ``average learner\\'\\' model computed above, was not effective in determining the outcome of a competency of an individual learner. This indicates that we cannot use one single aggregated model on all individuals. %There is a need to build individual models to understand the evidences better and improve the way of mapping learners to their competency. \\nModels must consider personalization i.e., we need to build individual models for each users and aggregate the models based on common properties.\\n\\nTo do this, we require significant amount of data for each learner. So, instead of aggregating time spent on resources at a competency level as total time, we looked at time spent for each resource and aggregated the data separately for each learner in the next experiment. \\n\\nExperiment 2:\\nIn this experiment, we used data from each activity event and modeled the time spent on each resource mapped to a particular competency. Using the start and the stop time, we computed the time spent on each resource and we also observed that some individuals consumed the same resource again. Based on this, we formed our second hypothesis: $H_u$: There is a large variance among the individual models built for each learner based on their learning activity data.\\n\\nWe wanted to observe the consumption of resources and the time spent on those resources, could predict the outcome of the corresponding competency for that individual learner. We also wanted to observe if the models built for each individual learner had common properties and could be merged to a single model or there is a large variance among them. \\n\\nThe amount of time spent on each resource is stored as a vector for each (learner,competency) pair, the length of the vector is the number of resources and the order of the vector tells the order in which the resources were consumed. The length of the vector varied for each (learner,competency) pair. To build individual learner models and compare them, we require equal number of features for all pairs of (learner, competency). \\n\\nTo achieve this we transformed the time spent on resource vector, to a matrix in the following way. We computed the range of total time spent on (resource,competency) pair by all learners. This value varied from 10 seconds to 46000 seconds. After observing this distribution, we decided to have a time frequency of 100 seconds and computed the cumulative sum of resources consumed by the learner at each frequency i.e. 100th second, 200th second etc.  We populated 460 time frequency columns. We arrived at this time frequency value of 100 seconds by dividing the maximum total time and time frequency. \\n\\nFor example, if the learner has consumed 2 resources for a competency code ``3\\'\\', spending 90 seconds on first resource and 170 seconds on second resource then the column 0(time_100) will have the value 1, column 1(time_200) will have the value 1 as the learner has not finished consuming the second resource by 200 seconds, column 2(time_300) has the value 2 and rest of the columns till column 459(time_460) will have the value 2 indicating that the user consumed maximum 2 resources. Fig.1, shows the subset of the data passed to the model for a single user, where code refers to competency code and rest of the columns are the evidences for that competency code for a single learner. The row containing code value 3, shows the corresponding result of example mentioned above. The features also includes normalized total_time spent on resources and normalized average_time spent on those resources for a particular (competency, learner) pair. \\n\\nFigure 1: Data passed to the classifier with cumulative frequency of resources with respect to time, total time and average time as features.\\n\\nSVM model with linear kernel was used to classify the data. Learners who had more than 30 competencies in any course and with any status (completed or in-progress), were selected for analysis. There were 42 learners who satisfied that criteria and individual SVM models were built for those learners. Individual learner\\'s data was divided into 80-20 split randomly for training and testing respectively. The models for each learner gave an accuracy between 80% to 100%. The accuracy was calculated from the confusion matrix generated for each individual model according to the formula mentioned in. Each model generated a coefficient vector of length 462. We also built a single model comprising the activity data all these 42 learners and called this the ``average-model\", as detailed in Experiment 1, for comparison. \\n\\nAll the 43 vectors of coefficients for the 43 models populated were clustered using the K-nearest neighbor clustering algorithm to observe any similarity among these models. The Euclidean distance between the data points (models) was used as a measure to cluster the models. To find the appropriate number of clusters, Elbow method was used to find the optimal number of clusters. The summation of distance between that specific cluster against the cluster centroid was computed and plotted for various number of clusters, between 1 and 43 where 43 was the total number of models.  The graph in Fig. 2 shows the variance in the sum of the squared distance between clusters against the number of clusters.\\n\\nFigure 2: Elbow method showing the variance of number of clusters and their distances\\n\\nUsing the variance in the Elbow method, we found the optimal number of clusters to be 6. This says that the 43 models including the average model can be clustered into 6 clusters, and there are some common properties among these models. \\n\\nFig.3 shows the distribution of 43 different models including the average model into 6 different clusters. The blue point refers to individual learner models and the red point indicates the ``average learner\\'\\' model. \\n\\nFigure 3: Distribution of individual learners models and average model into 6 clusters\\n\\nWe see that the individual models don\\'t cluster with the average model. For this dataset, the average model does not represent any individual learner. This validates our hypothesis that there is large variance among the individual models built for each learner based on their learning activity data and hence validates ``The Myth of the Average\". It also shows that we need not build one model for each individual which would make it extremely difficult to map new learners to their competencies. We find that there are some common properties among the learners which can be used to find the best model for each learner. We need to determine those common properties in the future and provide better learning experiences to each individual learner.\\n\\nConclusion:\\n\\nWe proposed an Evidence Based Competency Model, that uses data generated during the learning process by learners to find the outcome of the competency. The experiments shows that having a single average model can be tuned to get a higher accuracy, but does not represent any individual. This presses the need to consider individual learner\\'s variance and not rely on statistical averages to map learners to their competencies. This also says that we cannot provide each learner the same learning experience and cannot validate their underlying competencies through uniform evaluation mechanisms like outcomes. This research shows initial results towards that direction.\\n\\n5. ACKNOWLEDGEMENTS\\nThe authors would like to acknowledge and thank the con-\\ntributions of the project associates Thrivikram Mudunuri,\\nJuhi Singh, Naman Dosi and Kartik Gupta.\\n6. REFERENCES\\n[1] Todd Rose. The end of average: How to succeed in a\\nworld that values sameness . Penguin UK, 2016.\\n[2] L. Todd Rose, Parisa Rouhani, and Kurt W. Fischer.\\nThe science of the individual. Mind, Brain, and\\nEducation , 7(3), 2013.\\n[3] Maureen Tam. Outcomes-based approach to quality\\nassessment and curriculum improvement in higher\\neducation. Quality Assurance in Education ,\\n22(2):158{168, 2014.\\n[4] Laurie Brady. Outcome based education: a critique.\\nThe Curriculum Journal , 7(1):5{16, 1996.\\n[5] Praseeda, Srinath Srinivasa, and Prasad Ram.\\nValidating the myth of average through evidence based\\ncompetency model. Gooru Tech Report , 3(2), 2019.\\n[6] Stephen E. Fancsali, Guoguo Zheng, Yanyan Tan,\\nSteven Ritter, Susan R. Berman, and April Galyardt.\\nUsing embedded formative assessment to predict state\\nsummative test scores. LAK \\'18. ACM, 2018.\\n[7] Mingyu Feng, Neil T. Heffernan, and Kenneth R.\\nKoedinger. Predicting state test scores better with\\nintelligent tutoring systems: Developing metrics to\\nmeasure assistance required. Springer Berlin\\nHeidelberg, 2006.\\n[8] Kenneth R. Koedinger, Jihee Kim, Julianna Zhuxin\\nJia, Elizabeth A. McLaughlin, and Norman L. Bier.\\nLearning is not a spectator sport: Doing is better than\\nwatching for learning from a mooc. L@S \\'15, pages\\n111{120. ACM, 2015.\\n[9] J.D. Fletcher. Evidence for learning from\\ntechnology-assisted instruction. Technology\\nApplications in Education: A Learning View , pages\\n79{99, 2003.\\n[10] Corinna Cortes and Vladimir Vapnik. Support-vector\\nnetworks. In Machine Learning , pages 273{297, 1995.\\nThe 12th International Conference on Educational Data Mining\\n634\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f2=open(\"/sample(2)\",\"r\")\n",
        "d2=f2.read()\n",
        "f2.close()\n",
        "d2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "1zRewBCyIIVT",
        "outputId": "be48cba0-144a-4ffe-ec40-1ae12bf45f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Myth of Average is a belief that’s been prominent in most sciences and in education. It’s the belief that we can use statistical averages to understand individuals. Scientists have come to realize that it’s a myth, and over the last 10 years have been moving from averages to individuals, so for example we’re hearing a lot of things like ‘personalized medicine.’ Unfortunately, education has not quite realized the myth yet, and so what we have is a situation where not only do we accept the idea of designing something for the individual based on the average, we actually promote it. The myth is that the average is fits for most people, when, in fact, it doesn’t. When it comes to designing environments it hurts us in two ways. As I said in my TEDX talk, the first is that you can be incredibly talented in one area, but average or below average in another. For example, say you’re really gifted in math, but you are an average or below average reader. The way our education system is designed will make it very hard for us to be able to get at your talent, because even in math class many of the problems are reading problems, so the reading problem can mask what you’re really good at. The second way it hurts us is that someone can be unbelievably gifted in something, but their environment won’t challenge them because it’s teaching to the average. They end up getting on-board and doing only what they are supposed to. In this way, designing environments to averages end up hurting even our best and brightest.o me, the effect of the Myth of the Averages is even broader than education. It’s really about how we develop our current and future talent pool, and as you know we have big challenges in our society and need all of the talent and creativity we can get We already have all the raw talent that we need! If you think about something as big as finding a cure for cancer, we need as many people who have the talent and the work ethic becoming scientists and chasing down this problem as possible. But if we design our educational environment so that an individual’s limitations make it almost impossible for us to get to their talents, then we are going to lose a whole bunch of talented individuals, and in my mind we’re in danger of losing the cure for cancer. If we extend this myth of averages all the way it has very serious implications, because when we studied cancer on the average, it led us to conclusions that were not helping us actually cure people. And since we’ve gone away from average and started studying cancer, individual cancer, we’ve made great progress. The workforce environment is not dissimilar. We’re trying to get people to be the most productive and effective person they can be. But if the environment is designed around averages, it makes people less efficient, less creative. So, you know, in every sector of society this idea of average has turned out to be a sort of barrier to advancement.Well, to me it’s step one of a two-step process that gets us away from this average and toward helping our institutions become institutions of opportunity that can actually nurture individuals. So Ban the Average is the first step. It’s about helping people realize the average really is the problem. We can’t move forward until we realize that. \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f3=open(\"/sample(3)\",\"r\")\n",
        "d3=f3.read()\n",
        "f3.close()\n",
        "d3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "xA6fxkZ6IP2l",
        "outputId": "e83246e8-406a-4695-a507-743508218ad0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In cricket, a player's batting average is the total number of runs they have scored divided by the number of times they have been out, usually given to two decimal places. Since the number of runs a player scores and how often they get out are primarily measures of their own playing ability, and largely independent of their teammates, batting average is a good metric for an individual player's skill as a batter (although the practice of drawing comparisons between players on this basis is not without criticism[1]). The number is also simple to interpret intuitively. If all the batter's innings were completed (i.e. they were out every innings), this is the average number of runs they score per innings. If they did not complete all their innings (i.e. some innings they finished not out), this number is an estimate of the unknown average number of runs they score per innings.Each player normally has several batting averages, with a different figure calculated for each type of match they play (first-class, one-day, Test matches, List A, T20, etc.), and a player's batting averages may be calculated for individual seasons or series, or at particular grounds, or against particular opponents, or across their whole career.Batting average has been used to gauge cricket players' relative skills since the 18th century. Most players have career batting averages in the range of 20 to 40. This is also the desirable range for wicket-keepers, though some fall short and make up for it with keeping skill. Until a substantial increase in scores in the 21st century due to improved bats and smaller grounds among other factors, players who sustained an average above 50 through a career were considered exceptional, and before the development of the heavy roller in the 1870s (which allowed for a flatter, safer cricket pitch) an average of 25 was considered very good.Career records for batting average are usually subject to a minimum qualification of 20 innings played or completed, in order to exclude batsmen who have not played enough games for their skill to be reliably assessed. Under this qualification, the highest Test batting average belongs to Australia's Sir Donald Bradman, with 99.94. Given that a career batting average over 50 is exceptional, and that only 4 other players have averages over 60, this is an outstanding statistic. The fact that Bradman's average is so far above that of any other cricketer has led several statisticians to argue that, statistically at least, he was the greatest athlete in any sport.Career records for batting average are usually subject to a minimum qualification of 20 innings played or completed, in order to exclude batsmen who have not played enough games for their skill to be reliably assessed. Under this qualification, the highest Test batting average belongs to Australia's Sir Donald Bradman, with 99.94. Given that a career batting average over 50 is exceptional, and that only 4 other players have averages over 60, this is an outstanding statistic. The fact that Bradman's average is so far above that of any other cricketer has led several statisticians to argue that, statistically at least, he was the greatest athlete in any sport.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding1 = model.encode(d1, convert_to_tensor=True)\n",
        "embedding2 = model.encode(d2, convert_to_tensor=True)\n",
        "embedding3 = model.encode(d3, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "waIu2E-kI7-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores1 = util.pytorch_cos_sim(embedding1, embedding2)\n",
        "cosine_scores2 = util.pytorch_cos_sim(embedding1, embedding3)\n",
        "cosine_scores3 = util.pytorch_cos_sim(embedding2, embedding3)"
      ],
      "metadata": {
        "id": "6nP_CM2KJJnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6etDgBHJUZ_",
        "outputId": "3036a92a-3a40-4f12-f38d-312016b69ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5664]])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usT2RQ2yJXcO",
        "outputId": "808d2268-067d-47d9-d69b-de7d3af63b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3695]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Tvu2PngJb0y",
        "outputId": "e5d06933-f9c4-4850-9cd0-ecdd277c9774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4137]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores4 = util.pytorch_cos_sim(embedding1, embedding1)\n",
        "cosine_scores4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tvf2N7VgJjK2",
        "outputId": "9e2e2b90-2f5c-4217-bb5b-04759fc4599a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s4=\"Validating the myth of Average\"\n",
        "embedding4 = model.encode(s4, convert_to_tensor=True)\n",
        "cosine_scores7 = util.pytorch_cos_sim(embedding1, embedding4)\n",
        "cosine_scores8 = util.pytorch_cos_sim(embedding2, embedding4)\n",
        "cosine_scores9 = util.pytorch_cos_sim(embedding3, embedding4)"
      ],
      "metadata": {
        "id": "CcVrzBAMJ0t8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXBk9lDrKH10",
        "outputId": "31799e6f-119f-4b5e-8c2f-ad53727cf9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3448]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa2TeXQzKJ7X",
        "outputId": "e5899b50-df44-428b-8cd2-7dd48adc3d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3034]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(d3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfFkySEbKM00",
        "outputId": "73841634-264c-433f-d50d-f2707c896bcb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3207"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d1=d1[:len(d3)]\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "set_seed(42)\n",
        "d10=generator(d1, max_length=1000, num_return_sequences=1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJtd9Xh5o1Lf",
        "outputId": "26a38364-d656-491f-9c76-c6f0190dbdd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d11=d10['generated_text']\n",
        "\n",
        "d1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "sif0AzHms-Iy",
        "outputId": "6a64512c-c031-4b18-e5b2-e396ff5b94c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Validating The Myth of Average through Evidences\\nMost formal educational practices based on the classroom, imparts skills and knowledge at scale, by adopting the model of a factory. Here, the focus is on creation of formal processes, mass production of measurable outcomes, and standardization. Curricula and educational practices are designed for a hypothetical ``average\" student, having ``average\" abilities. However, recent advances in individualization have found vast discrepancies between  ind'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "TggQSjtywO9z",
        "outputId": "a8898dc7-6128-478d-80f9-5314f17c3422"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Validating The Myth of Average through Evidences\\nMost formal educational practices based on the classroom, imparts skills and knowledge at scale, by adopting the model of a factory. Here, the focus is on creation of formal processes, mass production of measurable outcomes, and standardization. Curricula and educational practices are designed for a hypothetical ``average\" student, having ``average\" abilities. However, recent advances in individualization have found vast discrepancies between  indy and high school graduates. In this series, I will argue empirically and methodically that the \\xa0model of a factory is wrong. In doing so, I provide a critical examination of the theory of typical education as applied to students in both the middle and high school levels. As a result, I take the role of a researcher in the field of education, with an emphasis on a range of fields including, but not limited to, biology, anthropology, mathematics, economics, social science, art, sociology, sociology and environmental psychology. This will provide a thorough introduction to this work.\\nIn Chapter 1 of this book, the title is the title of a speech I gave at the American Psychological Association National Convention in February 2005 entitled ``On Accumulation and Evaluation: A Practical Perspective\\'\\'. The title is derived from the British model of the ``acceleration of success.\\'\\' This means, in my judgment the goal would be to demonstrate empirically that the achievement rate (in our view at the time of this book\\'s publication) for any given achievement group is the same as that of the typical high school graduate and thus, ``that the average student, based on the model of a factory, does not have to have an average of the individual abilities he or she developed over an average of that work day.\\'\\' I am interested to develop a case in which the concept of ``average versus typical\\'\\' is true, which is the assumption of my own. The issue is not of the ability of one to demonstrate in a given field, but of the relationship between such attributes in practice. However, in my view, there has also been a change in the relationship among achievement groups over time, as the number of individuals in an effort to achieve that standard has increased. This is due to a variety of sources, but most of the cases mentioned in this book illustrate the importance of establishing a well-defined goal in the individual achievement process that provides an ideal picture of how the process may look if one has a simple goal of acquiring a few goals, like being able to pass the test.\\nThis book, by contrast, is concerned with establishing a system of common goals and a broad scope of achievements not necessarily related to individual efforts in the effort to achieve those goals. In my regard, the most pertinent of these goals is that for the average student, the goal given to him or her is usually a general, objective, measurable measure of success. On the other hand, if the achievement has a specific value for one\\'s own field of study, then the other goals may well depend on that of a group of students assigned by the training organization to perform those tasks. A group of students could be assigned a common goal for any one area, such as high school. Another useful method in the book is to define and describe standard values of the work performed by the students assigned for the same area. The goal will be a typical level of individual development while the individual achievement may be viewed as a general goal. In this case, one group of students might be assigned to run for president or run for the federal government when this goal is achieved as the group is a single achievement. Another group of students may be assigned to write a letter to their parents stating their desire for academic success and one other group might be assigned to do so under special circumstances. We can draw a clear line somewhere between goal attainment by a group of students and the goal attainment by a group of students. We might then decide on the value a student ought to attain by an action or in some other manner, such as being more effective, having a life and perhaps a good job. It is also important to make very clear to students that a goal attainment based on individual achievement is not the main goal in the effort to achieve it. Students will be different from those who achieve their goal by simply trying to get a higher level of achievement. They may attain some level without pursuing the desired level because they don\\'t seek to achieve the particular goal. For the sake of simplicity, those students with the most achievement may be the ones who most benefit the most from these high levels of achievement. It is thus understandable that the level or type of achievement that students do attain should not be determined by the group goals, but rather by how best to achieve them.\\nSince I think that most people are born with a level of success in all areas of life and as such, they can be expected to follow their own goals well into adulthood. Although the main goal in life will'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d11=d11[len(d1):len(d11)]\n",
        "d11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "DweSj7XFtopX",
        "outputId": "e16d4f72-6fde-4bf3-848c-96864adb66d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"y and high school graduates. In this series, I will argue empirically and methodically that the \\xa0model of a factory is wrong. In doing so, I provide a critical examination of the theory of typical education as applied to students in both the middle and high school levels. As a result, I take the role of a researcher in the field of education, with an emphasis on a range of fields including, but not limited to, biology, anthropology, mathematics, economics, social science, art, sociology, sociology and environmental psychology. This will provide a thorough introduction to this work.\\nIn Chapter 1 of this book, the title is the title of a speech I gave at the American Psychological Association National Convention in February 2005 entitled ``On Accumulation and Evaluation: A Practical Perspective''. The title is derived from the British model of the ``acceleration of success.'' This means, in my judgment the goal would be to demonstrate empirically that the achievement rate (in our view at the time of this book's publication) for any given achievement group is the same as that of the typical high school graduate and thus, ``that the average student, based on the model of a factory, does not have to have an average of the individual abilities he or she developed over an average of that work day.'' I am interested to develop a case in which the concept of ``average versus typical'' is true, which is the assumption of my own. The issue is not of the ability of one to demonstrate in a given field, but of the relationship between such attributes in practice. However, in my view, there has also been a change in the relationship among achievement groups over time, as the number of individuals in an effort to achieve that standard has increased. This is due to a variety of sources, but most of the cases mentioned in this book illustrate the importance of establishing a well-defined goal in the individual achievement process that provides an ideal picture of how the process may look if one has a simple goal of acquiring a few goals, like being able to pass the test.\\nThis book, by contrast, is concerned with establishing a system of common goals and a broad scope of achievements not necessarily related to individual efforts in the effort to achieve those goals. In my regard, the most pertinent of these goals is that for the average student, the goal given to him or her is usually a general, objective, measurable measure of success. On the other hand, if the achievement has a specific value for one's own field of study, then the other goals may well depend on that of a group of students assigned by the training organization to perform those tasks. A group of students could be assigned a common goal for any one area, such as high school. Another useful method in the book is to define and describe standard values of the work performed by the students assigned for the same area. The goal will be a typical level of individual development while the individual achievement may be viewed as a general goal. In this case, one group of students might be assigned to run for president or run for the federal government when this goal is achieved as the group is a single achievement. Another group of students may be assigned to write a letter to their parents stating their desire for academic success and one other group might be assigned to do so under special circumstances. We can draw a clear line somewhere between goal attainment by a group of students and the goal attainment by a group of students. We might then decide on the value a student ought to attain by an action or in some other manner, such as being more effective, having a life and perhaps a good job. It is also important to make very clear to students that a goal attainment based on individual achievement is not the main goal in the effort to achieve it. Students will be different from those who achieve their goal by simply trying to get a higher level of achievement. They may attain some level without pursuing the desired level because they don't seek to achieve the particular goal. For the sake of simplicity, those students with the most achievement may be the ones who most benefit the most from these high levels of achievement. It is thus understandable that the level or type of achievement that students do attain should not be determined by the group goals, but rather by how best to achieve them.\\nSince I think that most people are born with a level of success in all areas of life and as such, they can be expected to follow their own goals well into adulthood. Although the main goal in life will\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding5 = model.encode(d11, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "lPrd5RIqtI44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_scores10 = util.pytorch_cos_sim(embedding5, embedding1)\n",
        "cosine_scores10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_Y1CCLqtNzb",
        "outputId": "6d088d24-dd34-4606-8f63-9462a6f48bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5434]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}