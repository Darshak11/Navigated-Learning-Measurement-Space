{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69db2e24",
   "metadata": {},
   "source": [
    "# Implementing Rake-NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b5d5358",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rake-nltk in c:\\users\\pkrit\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in c:\\users\\pkrit\\anaconda3\\lib\\site-packages (from rake-nltk) (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pkrit\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pkrit\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\pkrit\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pkrit\\anaconda3\\lib\\site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pkrit\\anaconda3\\lib\\site-packages (from click->nltk<4.0.0,>=3.6.2->rake-nltk) (0.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e7823b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pkrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pkrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3901577",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4107e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rapid automatic key word extraction',\n",
       " 'many algorithms available',\n",
       " 'feature extraction',\n",
       " 'feature extraction',\n",
       " 'one',\n",
       " 'help',\n",
       " 'complex']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text=\"Feature extraction is not that complex. There are many algorithms available that can help you with feature extraction. Rapid Automatic Key Word Extraction is one of those\"\n",
    "r.extract_keywords_from_text(text)\n",
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b0496e",
   "metadata": {},
   "source": [
    "### Getting phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9f3e89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(23.0, 'rapid automatic key word extraction'),\n",
       " (9.0, 'many algorithms available'),\n",
       " (5.0, 'feature extraction'),\n",
       " (5.0, 'feature extraction'),\n",
       " (1.0, 'one'),\n",
       " (1.0, 'help'),\n",
       " (1.0, 'complex')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26214003",
   "metadata": {},
   "source": [
    "### Using on sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a977fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt', 'r') as file:\n",
    "    data = file.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190c71cb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating The Myth of Average through Evidences Most formal educational practices based on the classroom, imparts skills and knowledge at scale, by adopting the model of a factory. Here, the focus is on creation of formal processes, mass production of measurable outcomes, and standardization. Curricula and educational practices are designed for a hypothetical ``average\" student, having ``average\" abilities. However, recent advances in individualization have found vast discrepancies between  individual traits and group averages. Designing models based on group averages, are usually ineffective when the individual is the target beneficiary. This research proposes Evidence Based Competency Model as a mechanism for providing individualized learning experiences. Here, the term evidence, refers to informal data generated by the learner, pertinent to the learning process. Individual models are built for each learner, based on their learning activities. These models are clustered to observe  similarity in learning patterns among the individual learners. This study also shows significant variability from the ``average model\", validating The Myth of Average.  Keywords: Evidence, Average Model, Individualization, Evidence Based Competency Model, Learning Process. Introduction:  The objective of education is to create empowered individuals that are capable of problem solving, upholding their individuality, and possessing an ability to acquire relevant competencies in their area of interests. Standardization of learning practices based on the classroom, has lead to uniform teaching and assessment practices without regard to how disparate individuals behave, learn, develop and apply their knowledge.   The classroom model is able to provide pedagogical solutions at scale by addressing the needs of a hypothetical ``average'' individual. However, research on individualization have shown that, as the number of dimensions of concern increase, the probability of finding an individual who is average on all dimensions rapidly diminishes to zero. This is popularly called The Myth of the Average.  A uniform model based on statistical averages does not capture the patterns of variability among individual learners.  Each individual has different interests, disposition, contexts and styles to learn different topics.  One of the emerging approaches towards standardizing pedagogy models, is to focus on learning outcomes. The Outcome Based Education (OBE) model focuses on learner's ability to produce specific, measurable outcomes as part of the learning process. The emphasis on visible outcomes, discounts the holistic nature of education comprising of a number of tacit elements and OBE is also considered to be strongly rooted in behaviorist learning practices, that are incompatible with other learning cultures like constructivist  education.  In this work, we show that data generated during the learning process, referred to as evidence, can be used to reason about the underlying competency. We also show that one single average model to predict the state of the competency cannot be used and we need to build separate models for learners, hence validating the Myth of Average, discussed in more detail,  in.  The use of activity data, rather than assessments and outcomes, for determining learner's latent competency levels, have been addressed for specific activities in. The focus here has been to correlate different types of learning activities like watching video or learning by doing activities, to their implication on learning.   In this work, we don't focus on any specific learning activity and follow a generic approach to collect any kind of learning data and use machine learning techniques to correlate characteristics of activity data with data from outcomes and formal assessments. We focus on activities involving learners consuming resources like videos, text books, articles, documents in the current implementation of the model.  Evidence Modeling: The term ``evidence'' is contrasted with ``outcomes'' as follows: outcomes refer to assessment data collected from formal testing environments, where the learner is fully cognizant of being assessed and has explicitly prepared for the same. In contrast, evidence pertains to data collected on an implicit, continuous basis on any activity of the learner pertaining to the competency in question. Existing methods of determining the underlying competency of a learner through formal assessments and visible outcomes, has its own issues.  There is a need for models that uses evidences, based on the learner's activities and maps these learners to their competencies.  We propose a model that explores the activity data of the learners, generated while achieving the competency. The data, collected implicitly in a continuous fashion, is modeled to map the learners to their competencies.  Evidences can be considered analogous to an observation that an individual makes, in the classroom through interaction or silent observation of reactions, which contains significant insights. An offline system like a classroom is unable to gather data for each individual during the learning process, but when learning happens in a Technology Assisted Learning Environment(TALE), data is continuously captured by the platform. Technology augmentation can happen in various ways -- like sensors and RFID tags to record attendance, recording and analysis of students' classroom activities like questions, discussions, etc. We want to focus on the evidences, so that we dont have to rely only on the formal assessments to determine the competency of the learner.   A competency model based on evidences can also be used to identify anomalous cases where the outcomes state that the individual has the competency while the evidence indicates a lack of competency, or vice-versa. A teacher can only analyze such cases and address them appropriately. The model based on evidences, acts as a way of aiding the teacher in teaching and evaluating process. % It can also help the students identify areas they are weak in, and help them acquire the competency.  Newer learning domains like training drivers to learn a particular language that would improve their communication skills, may not have standardized competency models to aid in pedagogy and assessments. In such cases, models based on evidences would help to map the learners to their competencies.   The main challenge is to create an evidence model for collecting data, and to argue for the completeness of the evidence. Learning may happen outside of the evidence gathering, and different kinds of learning activities may require different kinds of evidences to reason about them.   In order to address the above challenge, we adopt a least-biased model for evidence modeling, and treat each form of evidence as equally important in the input feature vector. All forms of evidences collected are then given as input to a machine learning algorithm to find the best possible indicators for the outcomes based on assessment scores.   Experiments and Initial Results:  The activity data used to build models is collected from a large, open online learning platform, implemented across several schools in the US. The platform has aggregated open learning resources for various courses, provided by content creators, curators and instructors. The learning resource can be a document, video, audio, puzzles or any content used to obtain the competency. Learners enroll to various courses.   A course is organized into several competencies, where a competency is seen as the basic unit of learning. Each competency may have several learning resources mapped to it. Students consume learning resources and whenever they are ready, give assessments to earn a score. Instructors evaluate the assessments and provide their feedback in the form of scores. The scores indicate the status of the learner with respect to the competency. The learning resources are mapped to competencies for various courses like Maths, Science, English and Social Science in the K-12 curriculum.  %Each competency also has a signature assessment that the learner has to take in order to earn a status for that competency.   The activity data collected for a (learner, competency) pair, is divided into collections and assessments. Collections are resources used during the learning process, while assessments act as indicators of learning. In the platform, a learner is said to have achieved a ``completed\" status for a competency if one gets more than 80% in the respective assessments. The platform does not award a ``fail\" status to the learner. The learner keeps attempting assessments multiple times until the learner gets 80% or more. The status is then set to ``completed\", else the status is marked as ``in-progress\". %The data used to build Evidence based competency model contains learners who have completed status as well as in-progress status with respect to a particular competency.  Whenever a learner consumes a resource to learn a concept, an event is logged in the system with the details of the resource, time of the event, learner details and type of event(started consuming the resource). The same process is repeated when the learner finishes consuming the resource with a stop event. The events are captured for all the courses. Individuals consume various resources mapped to the same competency and at the end give assessments to get a particular status for that competency.    Using these activity data we have built a model to determine the outcome of the competency based on the evidences.  Experiment 1:  We built a Support Vector Machine (SVM) classifier to test a hypothesis. Our hypothesis is as follows: Ht: The time spent on learning resources, the total number of resources consumed by learners, can predict the outcome for the underlying competency.  The features identified are total time spent on resources, average time spent on resources and number of resources used for acquiring the corresponding competency. The users were given a completed or in-progress status based on their assessment scores. That serves as the ground truth for our model.   The dataset has 28000 (user,competency) pairs with their corresponding evidences. This data was divided into 80-20 split randomly for training and testing the SVM model respectively. We built a single SVM model for all learners and their competencies in all courses. We classified the data using linear, polynomial, sigmoid and radial basis kernel function. The data was not linearly separable. The radial basis kernel function was found to be the best kernel function to classify the data in terms of Accuracy, Precision and F1 score as shown in Table 1.   Kernel functions Accuracy Precision F1 score Radial basis 82.43% 68.79% 51.13% Linear 78.39% 53.65% 40.30% Polynomial 78.83% 56.25% 37.91% Sigmoid 68.84% 30.68% 30.37%  Table 1: Performance metrics comparing the kernels  The accuracy measure of the SVM model using the radial basis function states that using total time spent, average time spent on the resources and number of resources, we can significantly predict the outcome of the underlying competency. The outcomes are determined by the scores of the assessments, which was not used as feature to build the models. This shows that evidence can be used as an alternate way to model the outcome of the underlying competency.   The same model was made to classify all the competencies of a random individual learner, the accuracy of the average SVM model varied between 30% to 100% for different learners . The ``average learner'' model computed above, was not effective in determining the outcome of a competency of an individual learner. This indicates that we cannot use one single aggregated model on all individuals. %There is a need to build individual models to understand the evidences better and improve the way of mapping learners to their competency.  Models must consider personalization i.e., we need to build individual models for each users and aggregate the models based on common properties.  To do this, we require significant amount of data for each learner. So, instead of aggregating time spent on resources at a competency level as total time, we looked at time spent for each resource and aggregated the data separately for each learner in the next experiment.   Experiment 2: In this experiment, we used data from each activity event and modeled the time spent on each resource mapped to a particular competency. Using the start and the stop time, we computed the time spent on each resource and we also observed that some individuals consumed the same resource again. Based on this, we formed our second hypothesis: $H_u$: There is a large variance among the individual models built for each learner based on their learning activity data.  We wanted to observe the consumption of resources and the time spent on those resources, could predict the outcome of the corresponding competency for that individual learner. We also wanted to observe if the models built for each individual learner had common properties and could be merged to a single model or there is a large variance among them.   The amount of time spent on each resource is stored as a vector for each (learner,competency) pair, the length of the vector is the number of resources and the order of the vector tells the order in which the resources were consumed. The length of the vector varied for each (learner,competency) pair. To build individual learner models and compare them, we require equal number of features for all pairs of (learner, competency).   To achieve this we transformed the time spent on resource vector, to a matrix in the following way. We computed the range of total time spent on (resource,competency) pair by all learners. This value varied from 10 seconds to 46000 seconds. After observing this distribution, we decided to have a time frequency of 100 seconds and computed the cumulative sum of resources consumed by the learner at each frequency i.e. 100th second, 200th second etc.  We populated 460 time frequency columns. We arrived at this time frequency value of 100 seconds by dividing the maximum total time and time frequency.   For example, if the learner has consumed 2 resources for a competency code ``3'', spending 90 seconds on first resource and 170 seconds on second resource then the column 0(time_100) will have the value 1, column 1(time_200) will have the value 1 as the learner has not finished consuming the second resource by 200 seconds, column 2(time_300) has the value 2 and rest of the columns till column 459(time_460) will have the value 2 indicating that the user consumed maximum 2 resources. Fig.1, shows the subset of the data passed to the model for a single user, where code refers to competency code and rest of the columns are the evidences for that competency code for a single learner. The row containing code value 3, shows the corresponding result of example mentioned above. The features also includes normalized total_time spent on resources and normalized average_time spent on those resources for a particular (competency, learner) pair.   Figure 1: Data passed to the classifier with cumulative frequency of resources with respect to time, total time and average time as features.  SVM model with linear kernel was used to classify the data. Learners who had more than 30 competencies in any course and with any status (completed or in-progress), were selected for analysis. There were 42 learners who satisfied that criteria and individual SVM models were built for those learners. Individual learner's data was divided into 80-20 split randomly for training and testing respectively. The models for each learner gave an accuracy between 80% to 100%. The accuracy was calculated from the confusion matrix generated for each individual model according to the formula mentioned in. Each model generated a coefficient vector of length 462. We also built a single model comprising the activity data all these 42 learners and called this the ``average-model\", as detailed in Experiment 1, for comparison.   All the 43 vectors of coefficients for the 43 models populated were clustered using the K-nearest neighbor clustering algorithm to observe any similarity among these models. The Euclidean distance between the data points (models) was used as a measure to cluster the models. To find the appropriate number of clusters, Elbow method was used to find the optimal number of clusters. The summation of distance between that specific cluster against the cluster centroid was computed and plotted for various number of clusters, between 1 and 43 where 43 was the total number of models.  The graph in Fig. 2 shows the variance in the sum of the squared distance between clusters against the number of clusters.  Figure 2: Elbow method showing the variance of number of clusters and their distances  Using the variance in the Elbow method, we found the optimal number of clusters to be 6. This says that the 43 models including the average model can be clustered into 6 clusters, and there are some common properties among these models.   Fig.3 shows the distribution of 43 different models including the average model into 6 different clusters. The blue point refers to individual learner models and the red point indicates the ``average learner'' model.   Figure 3: Distribution of individual learners models and average model into 6 clusters  We see that the individual models don't cluster with the average model. For this dataset, the average model does not represent any individual learner. This validates our hypothesis that there is large variance among the individual models built for each learner based on their learning activity data and hence validates ``The Myth of the Average\". It also shows that we need not build one model for each individual which would make it extremely difficult to map new learners to their competencies. We find that there are some common properties among the learners which can be used to find the best model for each learner. We need to determine those common properties in the future and provide better learning experiences to each individual learner.  Conclusion:  We proposed an Evidence Based Competency Model, that uses data generated during the learning process by learners to find the outcome of the competency. The experiments shows that having a single average model can be tuned to get a higher accuracy, but does not represent any individual. This presses the need to consider individual learner's variance and not rely on statistical averages to map learners to their competencies. This also says that we cannot provide each learner the same learning experience and cannot validate their underlying competencies through uniform evaluation mechanisms like outcomes. This research shows initial results towards that direction.  5. ACKNOWLEDGEMENTS The authors would like to acknowledge and thank the con- tributions of the project associates Thrivikram Mudunuri, Juhi Singh, Naman Dosi and Kartik Gupta. 6. REFERENCES [1] Todd Rose. The end of average: How to succeed in a world that values sameness . Penguin UK, 2016. [2] L. Todd Rose, Parisa Rouhani, and Kurt W. Fischer. The science of the individual. Mind, Brain, and Education , 7(3), 2013. [3] Maureen Tam. Outcomes-based approach to quality assessment and curriculum improvement in higher education. Quality Assurance in Education , 22(2):158{168, 2014. [4] Laurie Brady. Outcome based education: a critique. The Curriculum Journal , 7(1):5{16, 1996. [5] Praseeda, Srinath Srinivasa, and Prasad Ram. Validating the myth of average through evidence based competency model. Gooru Tech Report , 3(2), 2019. [6] Stephen E. Fancsali, Guoguo Zheng, Yanyan Tan, Steven Ritter, Susan R. Berman, and April Galyardt. Using embedded formative assessment to predict state summative test scores. LAK '18. ACM, 2018. [7] Mingyu Feng, Neil T. Heffernan, and Kenneth R. Koedinger. Predicting state test scores better with intelligent tutoring systems: Developing metrics to measure assistance required. Springer Berlin Heidelberg, 2006. [8] Kenneth R. Koedinger, Jihee Kim, Julianna Zhuxin Jia, Elizabeth A. McLaughlin, and Norman L. Bier. Learning is not a spectator sport: Doing is better than watching for learning from a mooc. L@S '15, pages 111{120. ACM, 2015. [9] J.D. Fletcher. Evidence for learning from technology-assisted instruction. Technology Applications in Education: A Learning View , pages 79{99, 2003. [10] Corinna Cortes and Vladimir Vapnik. Support-vector networks. In Machine Learning , pages 273{297, 1995. The 12th International Conference on Educational Data Mining 634\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "820e8c96",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pkrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pkrit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3011a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "r = Rake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6baea772",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(data)\n",
    "out = r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "83f65c3d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(29.838709677419356,\n",
       "  'emerging approaches towards standardizing pedagogy models'),\n",
       " (28.285425101214575, 'learner keeps attempting assessments multiple times'),\n",
       " (28.120760959470637, 'competency models must consider personalization ie'),\n",
       " (28.117857142857144, 'row containing code value 3 shows'),\n",
       " (28.1, 'various courses like maths science english'),\n",
       " (26.73919413919414, 'competency code 3 spending 90 seconds'),\n",
       " (24.933333333333334, 'clusters figure 2 elbow method showing'),\n",
       " (24.76923076923077, 'features also includes normalized totaltime spent'),\n",
       " (22.80952380952381, 'average abilities however recent advances'),\n",
       " (22.35135916714864, 'average learner model figure 3 distribution'),\n",
       " (22.05446308077887, 'research proposes evidence based competency model'),\n",
       " (21.444444444444443, 'learning activities may require different kinds'),\n",
       " (21.257246376811594, 'courses individuals consume various resources mapped'),\n",
       " (21.208333333333332, 'research shows initial results towards'),\n",
       " (21.15, 'disparate individuals behave learn develop'),\n",
       " (21.11111111111111, 'learning cultures like constructivist education'),\n",
       " (20.42122132253711, 'outcome based education obe model focuses'),\n",
       " (20.23076923076923, 'uniform evaluation mechanisms like outcomes'),\n",
       " (20.11111111111111, 'learning activities like watching video'),\n",
       " (20.001923076923077, 'cannot use one single aggregated model'),\n",
       " (19.723502304147466, 'hypothetical average individual however research'),\n",
       " (19.350694444444443, 'technology assisted learning environmenttale data'),\n",
       " (19.041666666666664, 'study also shows significant variability'),\n",
       " (18.739583333333336, 'data using linear polynomial sigmoid'),\n",
       " (17.678787878787876, 'populated 460 time frequency columns'),\n",
       " (17.67857142857143, 'support vector machine svm classifier'),\n",
       " (17.58730158730159, 'evidence learning may happen outside'),\n",
       " (17.141341256366726, 'group averages designing models based'),\n",
       " (16.874643874643873, 'determining learners latent competency levels'),\n",
       " (16.5, 'various ways like sensors'),\n",
       " (16.5, 'document video audio puzzles'),\n",
       " (15.5, 'radial basis function states'),\n",
       " (15.5, 'knearest neighbor clustering algorithm'),\n",
       " (14.944444444444445, 'providing individualized learning experiences'),\n",
       " (14.833333333333334, 'formal processes mass production'),\n",
       " (14.833333333333334, 'columns till column 459time460'),\n",
       " (14.75, 'radial basis kernel function'),\n",
       " (14.75, 'radial basis kernel function'),\n",
       " (14.61111111111111, 'use machine learning techniques'),\n",
       " (14.5, 'different interests disposition contexts'),\n",
       " (14.357142857142858, '200 seconds column 2time300'),\n",
       " (14.05, 'learn different topics one'),\n",
       " (14.0, 'assessment practices without regard'),\n",
       " (13.618357487922705, 'students consume learning resources'),\n",
       " (13.1, 'next experiment experiment 2'),\n",
       " (13.042857142857143, 'value 1 column 1time200'),\n",
       " (12.785024154589372, 'aggregated open learning resources'),\n",
       " (12.478113553113552, 'one single average model'),\n",
       " (12.444444444444445, 'provide better learning experiences'),\n",
       " (12.339904420549582, 'variability among individual learners'),\n",
       " (12.172043010752688, '43 different models including'),\n",
       " (12.125618706263868, 'consider individual learners variance'),\n",
       " (12.085964912280703, 'formal educational practices based'),\n",
       " (11.98076923076923, 'produce specific measurable outcomes'),\n",
       " (11.648351648351648, 'average svm model varied'),\n",
       " (11.368357487922705, 'several learning resources mapped'),\n",
       " (10.721129747445538, 'evidence based competency model'),\n",
       " (10.613799283154123, 'learning process individual models'),\n",
       " (10.444444444444445, 'machine learning algorithm'),\n",
       " (10.433333333333334, 'various courses provided'),\n",
       " (10.298122167687385, 'resources average time spent'),\n",
       " (10.26923076923077, 'normalized averagetime spent'),\n",
       " (10.265219500363813, 'build individual learner models'),\n",
       " (10.002688172043012, 'individual models dont cluster'),\n",
       " (10.0, 'communication skills may'),\n",
       " (9.875, 'fig 2 shows'),\n",
       " (9.80952380952381, 'hypothetical average student'),\n",
       " (9.666666666666668, 'offline system like'),\n",
       " (9.666666666666666, 'require significant amount'),\n",
       " (9.666666666666666, 'contains significant insights'),\n",
       " (9.666666666666666, 'best kernel function'),\n",
       " (9.642857142857142, 'value 2 indicating'),\n",
       " (9.505413679808841, 'learners individual learners data'),\n",
       " (9.5, 'students identify areas'),\n",
       " (9.433333333333334, 'clusters elbow method'),\n",
       " (9.297161172161172, 'single svm model'),\n",
       " (9.284692500481974, 'average learner model computed'),\n",
       " (9.257875457875457, 'build one model'),\n",
       " (9.25, 'implicit continuous basis'),\n",
       " (9.213709677419356, 'models fig3 shows'),\n",
       " (9.197076023391814, 'learning practices based'),\n",
       " (9.172161172161172, 'features svm model'),\n",
       " (9.172043010752688, '43 models populated'),\n",
       " (9.144444444444446, 'behaviorist learning practices'),\n",
       " (9.129554655870445, 'uniform model based'),\n",
       " (9.005494505494505, 'svm model using'),\n",
       " (9.0, 'record attendance recording'),\n",
       " (9.0, 'question existing methods'),\n",
       " (9.0, 'problem solving upholding'),\n",
       " (9.0, 'correlate different types'),\n",
       " (9.0, '8020 split randomly'),\n",
       " (9.0, '8020 split randomly'),\n",
       " (8.847926267281107, 'individual svm models'),\n",
       " (8.678113553113553, 'single average model'),\n",
       " (8.672161172161172, 'svm model respectively'),\n",
       " (8.672043010752688, '43 models including'),\n",
       " (8.666666666666666, 'blue point refers'),\n",
       " (8.642857142857142, 'contrast evidence pertains'),\n",
       " (8.571428571428571, 'require equal number'),\n",
       " (8.55299539170507, 'build separate models'),\n",
       " (8.5, 'provide pedagogical solutions'),\n",
       " (8.5, 'platform technology augmentation'),\n",
       " (8.5, 'content creators curators'),\n",
       " (8.488311688311688, 'time frequency value'),\n",
       " (8.4, 'create empowered individuals'),\n",
       " (8.368589743589745, 'single model comprising'),\n",
       " (8.333333333333334, 'identify anomalous cases'),\n",
       " (8.324234904880067, 'individual model according'),\n",
       " (8.314685314685313, 'aggregating time spent'),\n",
       " (8.285714285714285, 'large variance among'),\n",
       " (8.285714285714285, 'large variance among'),\n",
       " (8.285714285714285, 'large variance among'),\n",
       " (8.244959677419356, 'data points models'),\n",
       " (8.224674589700056, 'cases models based'),\n",
       " (8.194444444444445, 'learning process referred'),\n",
       " (8.153846153846153, 'ready give assessments'),\n",
       " (8.153846153846153, 'end give assessments'),\n",
       " (8.133640552995391, 'build individual models'),\n",
       " (8.133640552995391, 'build individual models'),\n",
       " (8.120760959470637, 'standardized competency models'),\n",
       " (8.116883116883116, 'maximum total time'),\n",
       " (8.11111111111111, 'specific learning activity'),\n",
       " (8.078272604588394, 'competency model based'),\n",
       " (8.0, 'red point indicates'),\n",
       " (8.0, 'dimensions rapidly diminishes'),\n",
       " (8.0, '28000 usercompetency pairs'),\n",
       " (7.944444444444445, 'learning patterns among'),\n",
       " (7.90625, 'uses data generated'),\n",
       " (7.90625, 'informal data generated'),\n",
       " (7.898785425101215, 'uses evidences based'),\n",
       " (7.886446886446887, 'model evidence modeling'),\n",
       " (7.886446886446887, 'average model validating'),\n",
       " (7.886113886113886, 'total time spent'),\n",
       " (7.886113886113886, 'total time spent'),\n",
       " (7.833333333333334, 'formal testing environments'),\n",
       " (7.822916666666666, 'activity data rather'),\n",
       " (7.809523809523808, 'term evidence refers'),\n",
       " (7.8, 'common properties among'),\n",
       " (7.8, 'common properties among'),\n",
       " (7.767361111111111, 'learning activity data'),\n",
       " (7.767361111111111, 'learning activity data'),\n",
       " (7.739583333333334, 'data collected implicitly'),\n",
       " (7.730769230769231, 'visible outcomes discounts'),\n",
       " (7.730769230769231, 'follows outcomes refer'),\n",
       " (7.712224108658743, 'random individual learner'),\n",
       " (7.712224108658743, 'individual learner conclusion'),\n",
       " (7.678614097968937, 'individual learners models'),\n",
       " (7.673913043478261, 'consumed 2 resources'),\n",
       " (7.666666666666667, 'found vast discrepancies'),\n",
       " (7.666666666666666, 'best possible indicators'),\n",
       " (7.662337662337663, 'time total time'),\n",
       " (7.615384615384615, 'particular competency using'),\n",
       " (7.6, 'classroom imparts skills'),\n",
       " (7.6, '6 different clusters'),\n",
       " (7.550933786078098, 'individual learner models'),\n",
       " (7.541310541310541, 'competency learners enroll'),\n",
       " (7.539583333333333, 'assessment data collected'),\n",
       " (7.5, 'score instructors evaluate'),\n",
       " (7.5, 'different kinds'),\n",
       " (7.5, 'confusion matrix generated'),\n",
       " (7.464912280701755, 'learner finishes consuming'),\n",
       " (7.433333333333334, 'various courses'),\n",
       " (7.333333333333333, 'elbow method'),\n",
       " (7.15625, 'activity data collected'),\n",
       " (7.1, 'assessment scores experiments'),\n",
       " (7.0925925925925934, 'learners hence validating'),\n",
       " (7.086021505376345, 'individual models built'),\n",
       " (7.086021505376345, 'individual models built'),\n",
       " (7.046153846153846, 'evidences experiment 1'),\n",
       " (7.0092592592592595, 'map new learners'),\n",
       " (7.0, 'initial results'),\n",
       " (6.833333333333334, 'second hypothesis hu'),\n",
       " (6.782051282051282, 'particular competency whenever'),\n",
       " (6.75, 'linear kernel'),\n",
       " (6.663630229419703, 'learner competency pair'),\n",
       " (6.642857142857142, 'value 2'),\n",
       " (6.5299043062200965, 'inprogress status based'),\n",
       " (6.5, 'acquire relevant competencies'),\n",
       " (6.3, 'one gets'),\n",
       " (6.298245614035088, 'learner gets 80'),\n",
       " (6.289583333333333, 'activity data used'),\n",
       " (6.282051282051282, 'competency may'),\n",
       " (6.266666666666667, 'code refers'),\n",
       " (6.25, 'input feature vector'),\n",
       " (6.25, 'group averages'),\n",
       " (6.2, 'educational practices'),\n",
       " (6.17948717948718, 'evidences would help'),\n",
       " (6.131578947368421, 'event learner details'),\n",
       " (6.0, 'social science'),\n",
       " (6.0, 'education comprising'),\n",
       " (5.944444444444445, 'learning activities'),\n",
       " (5.923913043478261, 'resources could predict'),\n",
       " (5.891341256366724, 'models based'),\n",
       " (5.8820512820512825, 'competency code'),\n",
       " (5.8820512820512825, 'competency code'),\n",
       " (5.875, 'experiments shows'),\n",
       " (5.875, 'also shows'),\n",
       " (5.868589743589744, 'single model'),\n",
       " (5.809523809523809, 'value varied'),\n",
       " (5.796221322537113, 'model based'),\n",
       " (5.7592592592592595, 'different learners'),\n",
       " (5.75, 'specific activities'),\n",
       " (5.553113553113553, 'average model'),\n",
       " (5.553113553113553, 'average model'),\n",
       " (5.553113553113553, 'average model'),\n",
       " (5.553113553113553, 'average model'),\n",
       " (5.553113553113553, 'average model'),\n",
       " (5.552995391705069, 'build models'),\n",
       " (5.542857142857143, 'value 1'),\n",
       " (5.5, 'column 0time100'),\n",
       " (5.5, 'cannot provide'),\n",
       " (5.410256410256411, 'best model'),\n",
       " (5.404761904761905, 'various number'),\n",
       " (5.4, 'individuals consumed'),\n",
       " (5.386446886446887, 'evidence model'),\n",
       " (5.354978354978355, 'average time'),\n",
       " (5.350694444444445, 'learning data'),\n",
       " (5.345454545454546, 'time frequency'),\n",
       " (5.345454545454546, 'time frequency'),\n",
       " (5.334682860998651, 'competency based'),\n",
       " (5.314685314685315, 'time spent'),\n",
       " (5.314685314685315, 'time spent'),\n",
       " (5.314685314685315, 'time spent'),\n",
       " (5.314685314685315, 'time spent'),\n",
       " (5.314685314685315, 'time spent'),\n",
       " (5.314685314685315, 'time spent'),\n",
       " (5.314685314685315, 'time spent'),\n",
       " (5.2834008097166, 'outcomes based'),\n",
       " (5.282051282051282, 'competency cannot'),\n",
       " (5.2592592592592595, 'learners activities'),\n",
       " (5.256578947368421, 'single learner'),\n",
       " (5.243589743589744, 'model generated'),\n",
       " (5.230769230769231, 'measurable outcomes'),\n",
       " (5.199999999999999, 'experiment 1'),\n",
       " (5.194444444444445, 'learning process'),\n",
       " (5.194444444444445, 'learning process'),\n",
       " (5.194444444444445, 'learning process'),\n",
       " (5.194444444444445, 'learning process'),\n",
       " (5.184210526315789, 'learner based'),\n",
       " (5.184210526315789, 'learner based'),\n",
       " (5.184210526315789, 'learner based'),\n",
       " (5.1752136752136755, 'learning outcomes'),\n",
       " (5.142857142857142, 'term evidence'),\n",
       " (5.142857142857142, 'evidence modeling'),\n",
       " (5.125, 'single user'),\n",
       " (5.118357487922705, 'learning resources'),\n",
       " (5.118357487922705, 'learning resources'),\n",
       " (5.116883116883117, 'total time'),\n",
       " (5.115384615384615, 'competency using'),\n",
       " (5.0, 'specific cluster'),\n",
       " (5.0, 'similarity among'),\n",
       " (5.0, 'interests standardization'),\n",
       " (5.0, 'features identified'),\n",
       " (5.0, 'cannot validate'),\n",
       " (4.987179487179487, 'formal assessments'),\n",
       " (4.987179487179487, 'formal assessments'),\n",
       " (4.987179487179487, 'formal assessments'),\n",
       " (4.944444444444445, 'learning happens'),\n",
       " (4.944444444444445, 'learning experience'),\n",
       " (4.916666666666666, 'vector varied'),\n",
       " (4.90625, 'data generated'),\n",
       " (4.857142857142858, '46000 seconds'),\n",
       " (4.857142857142858, '170 seconds'),\n",
       " (4.857142857142858, '10 seconds'),\n",
       " (4.846153846153847, 'evidences better'),\n",
       " (4.839904420549582, 'individual learners'),\n",
       " (4.833333333333334, 'uniform teaching'),\n",
       " (4.833333333333334, 'distances using'),\n",
       " (4.822916666666666, 'activity data'),\n",
       " (4.822916666666666, 'activity data'),\n",
       " (4.822916666666666, 'activity data'),\n",
       " (4.822916666666666, 'activity data'),\n",
       " (4.809523809523809, 'average discussed'),\n",
       " (4.8, 'signature assessment'),\n",
       " (4.8, 'cumulative frequency'),\n",
       " (4.8, 'common properties'),\n",
       " (4.8, 'common properties'),\n",
       " (4.8, 'common properties'),\n",
       " (4.782051282051282, 'competency also'),\n",
       " (4.7592592592592595, 'learners generated'),\n",
       " (4.75, 'statistical averages'),\n",
       " (4.75, 'statistical averages'),\n",
       " (4.75, 'resource mapped'),\n",
       " (4.743589743589744, 'leastbiased model'),\n",
       " (4.739583333333334, 'data collected'),\n",
       " (4.730769230769231, 'visible outcomes'),\n",
       " (4.712224108658743, 'individual learner'),\n",
       " (4.712224108658743, 'individual learner'),\n",
       " (4.712224108658743, 'individual learner'),\n",
       " (4.712224108658743, 'individual learner'),\n",
       " (4.6655092592592595, 'data learners'),\n",
       " (4.642857142857142, 'evidence indicates'),\n",
       " (4.642857142857142, 'evidence gathering'),\n",
       " (4.6, 'assessment scores'),\n",
       " (4.580645161290322, 'individual traits'),\n",
       " (4.580645161290322, 'individual makes'),\n",
       " (4.545454545454545, 'stop time'),\n",
       " (4.505376344086022, 'models built'),\n",
       " (4.5, 'testing respectively'),\n",
       " (4.5, 'several competencies'),\n",
       " (4.5, 'particular language'),\n",
       " (4.5, 'follows ht'),\n",
       " (4.5, 'f1 score'),\n",
       " (4.5, 'correlate characteristics'),\n",
       " (4.5, 'continuous fashion'),\n",
       " (4.5, 'also observed'),\n",
       " (4.5, 'also help'),\n",
       " (4.5, 'also considered'),\n",
       " (4.444444444444445, 'learning resource'),\n",
       " (4.416666666666666, 'activity event'),\n",
       " (4.413630229419703, 'learner competency'),\n",
       " (4.40625, 'gather data'),\n",
       " (4.40625, 'data separately'),\n",
       " (4.40625, 'data passed'),\n",
       " (4.40625, 'collecting data'),\n",
       " (4.357142857142858, '100 seconds'),\n",
       " (4.357142857142858, '100 seconds'),\n",
       " (4.343589743589744, 'classroom model'),\n",
       " (4.333333333333334, 'would make'),\n",
       " (4.333333333333334, 'finished consuming'),\n",
       " (4.333333333333334, 'eventstarted consuming'),\n",
       " (4.333333333333334, '43 vectors'),\n",
       " (4.282051282051282, 'underlying competency'),\n",
       " (4.282051282051282, 'underlying competency'),\n",
       " (4.282051282051282, 'underlying competency'),\n",
       " (4.282051282051282, 'underlying competency'),\n",
       " (4.282051282051282, 'underlying competency'),\n",
       " (4.282051282051282, 'corresponding competency'),\n",
       " (4.282051282051282, 'corresponding competency'),\n",
       " (4.282051282051282, 'competency level'),\n",
       " (4.2592592592592595, 'mapping learners'),\n",
       " (4.2592592592592595, '42 learners'),\n",
       " (4.2592592592592595, '42 learners'),\n",
       " (4.25, 'vector tells'),\n",
       " (4.25, 'resourcecompetency pair'),\n",
       " (4.25, 'learnercompetency pair'),\n",
       " (4.25, 'learnercompetency pair'),\n",
       " (4.25, 'evaluating process'),\n",
       " (4.25, 'coefficient vector'),\n",
       " (4.25, 'cluster centroid'),\n",
       " (4.2272727272727275, 'particular status'),\n",
       " (4.17948717948718, 'evidences collected'),\n",
       " (4.173913043478261, 'resources consumed'),\n",
       " (4.173913043478261, 'resources consumed'),\n",
       " (4.166666666666667, 'clustered using'),\n",
       " (4.166666666666667, 'also built'),\n",
       " (4.153846153846153, 'respective assessments'),\n",
       " (4.153846153846153, 'assessments act'),\n",
       " (4.142857142857143, 'total number'),\n",
       " (4.142857142857143, 'total number'),\n",
       " (4.131578947368421, 'learner pertinent'),\n",
       " (4.131578947368421, 'learner pertaining'),\n",
       " (4.131578947368421, 'learner gave'),\n",
       " (4.131578947368421, 'learner consumes'),\n",
       " (4.1, '6 clusters'),\n",
       " (4.1, '6 clusters'),\n",
       " (4.045454545454545, 'resource time'),\n",
       " (4.0092592592592595, 'map learners'),\n",
       " (4.0, 'usually ineffective'),\n",
       " (4.0, 'target beneficiary'),\n",
       " (4.0, 'tacit elements'),\n",
       " (4.0, 'strongly rooted'),\n",
       " (4.0, 'stop event'),\n",
       " (4.0, 'standardization curricula'),\n",
       " (4.0, 'rfid tags'),\n",
       " (4.0, 'linearly separable'),\n",
       " (4.0, 'k12 curriculum'),\n",
       " (4.0, 'holistic nature'),\n",
       " (4.0, 'hence validates'),\n",
       " (4.0, 'ground truth'),\n",
       " (4.0, 'generic approach'),\n",
       " (4.0, 'fully cognizant'),\n",
       " (4.0, 'formula mentioned'),\n",
       " (4.0, 'extremely difficult'),\n",
       " (4.0, 'explicitly prepared'),\n",
       " (4.0, 'equally important'),\n",
       " (4.0, 'direction 5'),\n",
       " (4.0, 'current implementation'),\n",
       " (4.0, 'corresponding result'),\n",
       " (4.0, 'considered analogous'),\n",
       " (4.0, 'concern increase'),\n",
       " (4.0, 'basic unit'),\n",
       " (4.0, 'also wanted'),\n",
       " (4.0, 'also show'),\n",
       " (4.0, 'also says'),\n",
       " (3.966666666666667, 'content used'),\n",
       " (3.872916666666667, 'used data'),\n",
       " (3.872916666666667, 'data used'),\n",
       " (3.8461538461538463, 'evidences acts'),\n",
       " (3.8461538461538463, 'corresponding evidences'),\n",
       " (3.8333333333333335, 'would improve'),\n",
       " (3.8333333333333335, 'second resource'),\n",
       " (3.8333333333333335, 'second resource'),\n",
       " (3.8, 'scores indicate'),\n",
       " (3.8, 'completed else'),\n",
       " (3.759259259259259, 'learners ability'),\n",
       " (3.75, 'significantly predict'),\n",
       " (3.75, 'resource vector'),\n",
       " (3.730769230769231, 'outcomes state'),\n",
       " (3.7272727272727275, 'fail status'),\n",
       " (3.666666666666667, 'squared distance'),\n",
       " (3.666666666666667, 'euclidean distance'),\n",
       " (3.6666666666666665, 'use'),\n",
       " (3.6666666666666665, 'aggregated'),\n",
       " (3.6538461538461537, 'assessments collections'),\n",
       " (3.6405797101449275, 'resources used'),\n",
       " (3.6405797101449275, 'resources used'),\n",
       " (3.6, 'courses'),\n",
       " (3.571428571428571, 'optimal number'),\n",
       " (3.571428571428571, 'optimal number'),\n",
       " (3.571428571428571, 'appropriate number'),\n",
       " (3.5272727272727273, 'status completed'),\n",
       " (3.5272727272727273, 'completed status'),\n",
       " (3.5272727272727273, 'completed status'),\n",
       " (3.5, 'underlying competencies'),\n",
       " (3.5, 'silent observation'),\n",
       " (3.5, 'popularly called'),\n",
       " (3.5, 'pedagogy'),\n",
       " (3.5, 'obe'),\n",
       " (3.5, 'main challenge'),\n",
       " (3.5, 'higher accuracy'),\n",
       " (3.5, 'following way'),\n",
       " (3.5, 'first resource'),\n",
       " (3.5, 'example mentioned'),\n",
       " (3.5, 'education'),\n",
       " (3.5, 'dont focus'),\n",
       " (3.5, 'cumulative sum'),\n",
       " (3.5, 'continuously captured'),\n",
       " (3.5, 'alternate way'),\n",
       " (3.5, 'accuracy precision'),\n",
       " (3.4772727272727275, 'inprogress status'),\n",
       " (3.4, 'individuals'),\n",
       " (3.375, 'shows'),\n",
       " (3.3461538461538463, 'competencies evidences'),\n",
       " (3.3333333333333335, 'columns'),\n",
       " (3.333333333333333, 'length 462'),\n",
       " (3.25, 'observe similarity'),\n",
       " (3.25, 'mapped'),\n",
       " (3.0526315789473686, 'based'),\n",
       " (3.0, 'happen'),\n",
       " (3.0, 'features'),\n",
       " (3.0, 'classifier'),\n",
       " (3.0, 'activities'),\n",
       " (3.0, 'accuracy measure'),\n",
       " (3.0, '30 competencies'),\n",
       " (2.9444444444444446, 'learning'),\n",
       " (2.9444444444444446, 'learning'),\n",
       " (2.9444444444444446, 'learning'),\n",
       " (2.9444444444444446, 'learning'),\n",
       " (2.838709677419355, 'models'),\n",
       " (2.838709677419355, 'models'),\n",
       " (2.838709677419355, 'models'),\n",
       " (2.838709677419355, 'models'),\n",
       " (2.838709677419355, 'models'),\n",
       " (2.838709677419355, 'models'),\n",
       " (2.838709677419355, 'models'),\n",
       " (2.8095238095238093, 'average'),\n",
       " (2.8095238095238093, 'average'),\n",
       " (2.8095238095238093, 'average'),\n",
       " (2.8095238095238093, 'average'),\n",
       " (2.8, 'experiment'),\n",
       " (2.75, 'learn'),\n",
       " (2.75, 'learn'),\n",
       " (2.7435897435897436, 'model'),\n",
       " (2.7435897435897436, 'model'),\n",
       " (2.7435897435897436, 'model'),\n",
       " (2.7435897435897436, 'model'),\n",
       " (2.7435897435897436, 'model'),\n",
       " (2.7435897435897436, 'model'),\n",
       " (2.7435897435897436, 'model'),\n",
       " (2.7142857142857144, 'build'),\n",
       " (2.6666666666666665, 'distribution'),\n",
       " (2.6666666666666665, 'distribution'),\n",
       " (2.642857142857143, 'evidence'),\n",
       " (2.642857142857143, 'evidence'),\n",
       " (2.642857142857143, 'evidence'),\n",
       " (2.5806451612903225, 'individual'),\n",
       " (2.5806451612903225, 'individual'),\n",
       " (2.5806451612903225, 'individual'),\n",
       " (2.5806451612903225, 'individual'),\n",
       " (2.5806451612903225, 'individual'),\n",
       " (2.5806451612903225, 'individual'),\n",
       " (2.5806451612903225, 'individual'),\n",
       " (2.5, 'provide'),\n",
       " (2.5, 'also'),\n",
       " (2.4166666666666665, 'activity'),\n",
       " (2.40625, 'data'),\n",
       " (2.40625, 'data'),\n",
       " (2.40625, 'data'),\n",
       " (2.40625, 'data'),\n",
       " (2.40625, 'data'),\n",
       " (2.4, '1'),\n",
       " (2.3333333333333335, 'validating'),\n",
       " (2.3333333333333335, 'dont'),\n",
       " (2.3333333333333335, 'determining'),\n",
       " (2.3333333333333335, 'determining'),\n",
       " (2.3333333333333335, 'collected'),\n",
       " (2.3333333333333335, 'cases'),\n",
       " (2.3333333333333335, '43'),\n",
       " (2.3333333333333335, '43'),\n",
       " (2.2857142857142856, 'variance'),\n",
       " (2.2857142857142856, 'variance'),\n",
       " (2.2857142857142856, 'variance'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.282051282051282, 'competency'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.259259259259259, 'learners'),\n",
       " (2.25, 'vector'),\n",
       " (2.25, 'vector'),\n",
       " (2.25, 'process'),\n",
       " (2.25, 'cluster'),\n",
       " (2.230769230769231, 'outcomes'),\n",
       " (2.230769230769231, 'outcomes'),\n",
       " (2.230769230769231, 'outcomes'),\n",
       " (2.230769230769231, 'outcomes'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1739130434782608, 'resources'),\n",
       " (2.1538461538461537, 'assessments'),\n",
       " (2.1538461538461537, 'assessments'),\n",
       " (2.1538461538461537, 'assessments'),\n",
       " (2.1538461538461537, 'assessments'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1315789473684212, 'learner'),\n",
       " (2.1, 'clusters'),\n",
       " (2.1, 'clusters'),\n",
       " (2.1, 'clusters'),\n",
       " (2.1, 'clusters'),\n",
       " (2.1, 'clusters'),\n",
       " (2.0, 'whenever'),\n",
       " (2.0, 'testing'),\n",
       " (2.0, 'system'),\n",
       " (2.0, 'patterns'),\n",
       " (2.0, 'pairs'),\n",
       " (2.0, 'matrix'),\n",
       " (2.0, 'instructors'),\n",
       " (2.0, 'input'),\n",
       " (2.0, 'indicators'),\n",
       " (2.0, 'indicates'),\n",
       " (2.0, 'help'),\n",
       " (2.0, 'feature'),\n",
       " (2.0, 'event'),\n",
       " (2.0, 'dimensions'),\n",
       " (2.0, 'details'),\n",
       " (2.0, 'create'),\n",
       " (2.0, 'could'),\n",
       " (2.0, 'consumed'),\n",
       " (2.0, 'amount'),\n",
       " (2.0, 'acquire'),\n",
       " (2.0, '6'),\n",
       " (1.8461538461538463, 'evidences'),\n",
       " (1.8461538461538463, 'evidences'),\n",
       " (1.8461538461538463, 'evidences'),\n",
       " (1.8461538461538463, 'evidences'),\n",
       " (1.8461538461538463, 'evidences'),\n",
       " (1.8, 'scores'),\n",
       " (1.8, 'scores'),\n",
       " (1.8, 'completed'),\n",
       " (1.75, 'predict'),\n",
       " (1.75, 'predict'),\n",
       " (1.75, 'map'),\n",
       " (1.75, 'map'),\n",
       " (1.75, 'inprogress'),\n",
       " (1.75, 'inprogress'),\n",
       " (1.7272727272727273, 'status'),\n",
       " (1.7272727272727273, 'status'),\n",
       " (1.7272727272727273, 'status'),\n",
       " (1.7272727272727273, 'status'),\n",
       " (1.6666666666666667, 'found'),\n",
       " (1.6666666666666667, 'found'),\n",
       " (1.6666666666666667, 'distance'),\n",
       " (1.6666666666666667, 'built'),\n",
       " (1.6666666666666667, 'built'),\n",
       " (1.6666666666666667, 'built'),\n",
       " (1.6666666666666667, 'built'),\n",
       " (1.6666666666666667, 'built'),\n",
       " (1.6666666666666667, '80'),\n",
       " (1.6666666666666667, '80'),\n",
       " (1.625, 'outcome'),\n",
       " (1.625, 'outcome'),\n",
       " (1.625, 'outcome'),\n",
       " (1.625, 'outcome'),\n",
       " (1.625, 'outcome'),\n",
       " (1.625, 'outcome'),\n",
       " (1.625, 'outcome'),\n",
       " (1.6, 'computed'),\n",
       " (1.6, 'computed'),\n",
       " (1.6, 'computed'),\n",
       " (1.6, 'computed'),\n",
       " (1.6, 'classroom'),\n",
       " (1.6, 'classroom'),\n",
       " (1.6, 'classroom'),\n",
       " (1.5714285714285714, 'number'),\n",
       " (1.5714285714285714, 'number'),\n",
       " (1.5714285714285714, 'number'),\n",
       " (1.5714285714285714, 'number'),\n",
       " (1.5714285714285714, 'number'),\n",
       " (1.5714285714285714, 'number'),\n",
       " (1.5714285714285714, 'number'),\n",
       " (1.5, 'way'),\n",
       " (1.5, 'way'),\n",
       " (1.5, 'wanted'),\n",
       " (1.5, 'validates'),\n",
       " (1.5, 'teaching'),\n",
       " (1.5, 'sum'),\n",
       " (1.5, 'state'),\n",
       " (1.5, 'show'),\n",
       " (1.5, 'says'),\n",
       " (1.5, 'resource'),\n",
       " (1.5, 'resource'),\n",
       " (1.5, 'resource'),\n",
       " (1.5, 'resource'),\n",
       " (1.5, 'resource'),\n",
       " (1.5, 'resource'),\n",
       " (1.5, 'resource'),\n",
       " (1.5, 'platform'),\n",
       " (1.5, 'platform'),\n",
       " (1.5, 'platform'),\n",
       " (1.5, 'observation'),\n",
       " (1.5, 'measure'),\n",
       " (1.5, 'improve'),\n",
       " (1.5, 'hypothesis'),\n",
       " (1.5, 'hypothesis'),\n",
       " (1.5, 'hypothesis'),\n",
       " (1.5, 'example'),\n",
       " (1.5, 'competencies'),\n",
       " (1.5, 'competencies'),\n",
       " (1.5, 'competencies'),\n",
       " (1.5, 'competencies'),\n",
       " (1.5, 'competencies'),\n",
       " (1.5, 'competencies'),\n",
       " (1.5, 'competencies'),\n",
       " (1.5, 'collections'),\n",
       " (1.5, 'challenge'),\n",
       " (1.5, 'captured'),\n",
       " (1.5, 'called'),\n",
       " (1.5, 'accuracy'),\n",
       " (1.5, 'accuracy'),\n",
       " (1.5, 'accuracy'),\n",
       " (1.5, 'ability'),\n",
       " (1.5, '30'),\n",
       " (1.5, '100'),\n",
       " (1.5, '100'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.4666666666666666, 'used'),\n",
       " (1.3333333333333333, 'length'),\n",
       " (1.3333333333333333, 'length'),\n",
       " (1.3333333333333333, 'clustered'),\n",
       " (1.3333333333333333, 'clustered'),\n",
       " (1.25, 'observe'),\n",
       " (1.25, 'observe'),\n",
       " (1.25, 'observe'),\n",
       " (1.1666666666666667, 'focus'),\n",
       " (1.1666666666666667, 'focus'),\n",
       " (1.1666666666666667, 'focus'),\n",
       " (1.1666666666666667, 'focus'),\n",
       " (1.1666666666666667, 'focus'),\n",
       " (1.0, 'zero'),\n",
       " (1.0, 'work'),\n",
       " (1.0, 'work'),\n",
       " (1.0, 'well'),\n",
       " (1.0, 'weak'),\n",
       " (1.0, 'want'),\n",
       " (1.0, 'viceversa'),\n",
       " (1.0, 'users'),\n",
       " (1.0, 'users'),\n",
       " (1.0, 'us'),\n",
       " (1.0, 'understand'),\n",
       " (1.0, 'unable'),\n",
       " (1.0, 'type'),\n",
       " (1.0, 'tuned'),\n",
       " (1.0, 'treat'),\n",
       " (1.0, 'transformed'),\n",
       " (1.0, 'training'),\n",
       " (1.0, 'training'),\n",
       " (1.0, 'test'),\n",
       " (1.0, 'terms'),\n",
       " (1.0, 'teacher'),\n",
       " (1.0, 'teacher'),\n",
       " (1.0, 'take'),\n",
       " (1.0, 'summation'),\n",
       " (1.0, 'subset'),\n",
       " (1.0, 'styles'),\n",
       " (1.0, 'stored'),\n",
       " (1.0, 'start'),\n",
       " (1.0, 'shown'),\n",
       " (1.0, 'shown'),\n",
       " (1.0, 'set'),\n",
       " (1.0, 'serves'),\n",
       " (1.0, 'selected'),\n",
       " (1.0, 'seen'),\n",
       " (1.0, 'see'),\n",
       " (1.0, 'scale'),\n",
       " (1.0, 'scale'),\n",
       " (1.0, 'satisfied'),\n",
       " (1.0, 'said'),\n",
       " (1.0, 'rest'),\n",
       " (1.0, 'rest'),\n",
       " (1.0, 'respect'),\n",
       " (1.0, 'respect'),\n",
       " (1.0, 'respect'),\n",
       " (1.0, 'represent'),\n",
       " (1.0, 'represent'),\n",
       " (1.0, 'repeated'),\n",
       " (1.0, 'rely'),\n",
       " (1.0, 'rely'),\n",
       " (1.0, 'reason'),\n",
       " (1.0, 'reason'),\n",
       " (1.0, 'reactions'),\n",
       " (1.0, 'range'),\n",
       " (1.0, 'proposed'),\n",
       " (1.0, 'propose'),\n",
       " (1.0, 'probability'),\n",
       " (1.0, 'presses'),\n",
       " (1.0, 'possessing'),\n",
       " (1.0, 'plotted'),\n",
       " (1.0, 'part'),\n",
       " (1.0, 'organized'),\n",
       " (1.0, 'order'),\n",
       " (1.0, 'order'),\n",
       " (1.0, 'order'),\n",
       " (1.0, 'order'),\n",
       " (1.0, 'obtain'),\n",
       " (1.0, 'observing'),\n",
       " (1.0, 'objective'),\n",
       " (1.0, 'needs'),\n",
       " (1.0, 'need'),\n",
       " (1.0, 'need'),\n",
       " (1.0, 'need'),\n",
       " (1.0, 'need'),\n",
       " (1.0, 'need'),\n",
       " (1.0, 'need'),\n",
       " (1.0, 'need'),\n",
       " (1.0, 'myth'),\n",
       " (1.0, 'myth'),\n",
       " (1.0, 'myth'),\n",
       " (1.0, 'myth'),\n",
       " (1.0, 'myth'),\n",
       " (1.0, 'modeled'),\n",
       " (1.0, 'modeled'),\n",
       " (1.0, 'merged'),\n",
       " (1.0, 'mechanism'),\n",
       " (1.0, 'marked'),\n",
       " (1.0, 'maps'),\n",
       " (1.0, 'made'),\n",
       " (1.0, 'looked'),\n",
       " (1.0, 'logged'),\n",
       " (1.0, 'lead'),\n",
       " (1.0, 'lack'),\n",
       " (1.0, 'knowledge'),\n",
       " (1.0, 'knowledge'),\n",
       " (1.0, 'kind'),\n",
       " (1.0, 'kernels'),\n",
       " (1.0, 'issues'),\n",
       " (1.0, 'interaction'),\n",
       " (1.0, 'instead'),\n",
       " (1.0, 'individualization'),\n",
       " (1.0, 'individualization'),\n",
       " (1.0, 'individuality'),\n",
       " (1.0, 'incompatible'),\n",
       " (1.0, 'implication'),\n",
       " (1.0, 'graph'),\n",
       " (1.0, 'given'),\n",
       " (1.0, 'given'),\n",
       " (1.0, 'get'),\n",
       " (1.0, 'get'),\n",
       " (1.0, 'future'),\n",
       " (1.0, 'forms'),\n",
       " (1.0, 'formed'),\n",
       " (1.0, 'form'),\n",
       " (1.0, 'form'),\n",
       " (1.0, 'follow'),\n",
       " (1.0, 'finding'),\n",
       " (1.0, 'find'),\n",
       " (1.0, 'find'),\n",
       " (1.0, 'find'),\n",
       " (1.0, 'find'),\n",
       " (1.0, 'find'),\n",
       " (1.0, 'find'),\n",
       " (1.0, 'feedback'),\n",
       " (1.0, 'factory'),\n",
       " (1.0, 'explores'),\n",
       " (1.0, 'events'),\n",
       " (1.0, 'emphasis'),\n",
       " (1.0, 'effective'),\n",
       " (1.0, 'earn'),\n",
       " (1.0, 'earn'),\n",
       " (1.0, 'dividing'),\n",
       " (1.0, 'divided'),\n",
       " (1.0, 'divided'),\n",
       " (1.0, 'divided'),\n",
       " (1.0, 'determined'),\n",
       " (1.0, 'determine'),\n",
       " (1.0, 'determine'),\n",
       " (1.0, 'determine'),\n",
       " (1.0, 'detailed'),\n",
       " (1.0, 'detail'),\n",
       " (1.0, 'designed'),\n",
       " (1.0, 'decided'),\n",
       " (1.0, 'dataset'),\n",
       " (1.0, 'dataset'),\n",
       " (1.0, 'criteria'),\n",
       " (1.0, 'creation'),\n",
       " (1.0, 'course'),\n",
       " (1.0, 'course'),\n",
       " (1.0, 'contrasted'),\n",
       " (1.0, 'consumption'),\n",
       " (1.0, 'concept'),\n",
       " (1.0, 'completeness'),\n",
       " (1.0, 'comparison'),\n",
       " (1.0, 'compare'),\n",
       " (1.0, 'collect'),\n",
       " (1.0, 'coefficients'),\n",
       " (1.0, 'classify'),\n",
       " (1.0, 'classify'),\n",
       " (1.0, 'classify'),\n",
       " (1.0, 'classified'),\n",
       " (1.0, 'capture'),\n",
       " (1.0, 'capable'),\n",
       " (1.0, 'calculated'),\n",
       " (1.0, 'award'),\n",
       " (1.0, 'averagemodel'),\n",
       " (1.0, 'assessed'),\n",
       " (1.0, 'arrived'),\n",
       " (1.0, 'argue'),\n",
       " (1.0, 'area'),\n",
       " (1.0, 'appropriately'),\n",
       " (1.0, 'apply'),\n",
       " (1.0, 'analyze'),\n",
       " (1.0, 'analysis'),\n",
       " (1.0, 'analysis'),\n",
       " (1.0, 'aiding'),\n",
       " (1.0, 'aid'),\n",
       " (1.0, 'aggregate'),\n",
       " (1.0, 'adopting'),\n",
       " (1.0, 'adopt'),\n",
       " (1.0, 'addressing'),\n",
       " (1.0, 'addressed'),\n",
       " (1.0, 'address'),\n",
       " (1.0, 'address'),\n",
       " (1.0, 'acquiring'),\n",
       " (1.0, 'achieving'),\n",
       " (1.0, 'achieved'),\n",
       " (1.0, 'achieve'),\n",
       " (1.0, 'able')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89bd2b",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1804c76",
   "metadata": {},
   "source": [
    "#### Removing Acknowledgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e068135",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = data\n",
    "before, separator, after = text.partition('ACKNOWLEDGEMENTS')\n",
    "# print(before)\n",
    "short_data = before\n",
    "# short_data = data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d8a3c1",
   "metadata": {},
   "source": [
    "#### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3cc8520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "new_data = short_data.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b1ee6d9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating The Myth of Average through Evidences Most formal educational practices based on the classroom imparts skills and knowledge at scale by adopting the model of a factory Here the focus is on creation of formal processes mass production of measurable outcomes and standardization Curricula and educational practices are designed for a hypothetical average student having average abilities However recent advances in individualization have found vast discrepancies between  individual traits and group averages Designing models based on group averages are usually ineffective when the individual is the target beneficiary This research proposes Evidence Based Competency Model as a mechanism for providing individualized learning experiences Here the term evidence refers to informal data generated by the learner pertinent to the learning process Individual models are built for each learner based on their learning activities These models are clustered to observe  similarity in learning patterns among the individual learners This study also shows significant variability from the average model validating The Myth of Average  Keywords Evidence Average Model Individualization Evidence Based Competency Model Learning Process Introduction  The objective of education is to create empowered individuals that are capable of problem solving upholding their individuality and possessing an ability to acquire relevant competencies in their area of interests Standardization of learning practices based on the classroom has lead to uniform teaching and assessment practices without regard to how disparate individuals behave learn develop and apply their knowledge   The classroom model is able to provide pedagogical solutions at scale by addressing the needs of a hypothetical average individual However research on individualization have shown that as the number of dimensions of concern increase the probability of finding an individual who is average on all dimensions rapidly diminishes to zero This is popularly called The Myth of the Average  A uniform model based on statistical averages does not capture the patterns of variability among individual learners  Each individual has different interests disposition contexts and styles to learn different topics  One of the emerging approaches towards standardizing pedagogy models is to focus on learning outcomes The Outcome Based Education OBE model focuses on learners ability to produce specific measurable outcomes as part of the learning process The emphasis on visible outcomes discounts the holistic nature of education comprising of a number of tacit elements and OBE is also considered to be strongly rooted in behaviorist learning practices that are incompatible with other learning cultures like constructivist  education  In this work we show that data generated during the learning process referred to as evidence can be used to reason about the underlying competency We also show that one single average model to predict the state of the competency cannot be used and we need to build separate models for learners hence validating the Myth of Average discussed in more detail  in  The use of activity data rather than assessments and outcomes for determining learners latent competency levels have been addressed for specific activities in The focus here has been to correlate different types of learning activities like watching video or learning by doing activities to their implication on learning   In this work we dont focus on any specific learning activity and follow a generic approach to collect any kind of learning data and use machine learning techniques to correlate characteristics of activity data with data from outcomes and formal assessments We focus on activities involving learners consuming resources like videos text books articles documents in the current implementation of the model  Evidence Modeling The term evidence is contrasted with outcomes as follows outcomes refer to assessment data collected from formal testing environments where the learner is fully cognizant of being assessed and has explicitly prepared for the same In contrast evidence pertains to data collected on an implicit continuous basis on any activity of the learner pertaining to the competency in question Existing methods of determining the underlying competency of a learner through formal assessments and visible outcomes has its own issues  There is a need for models that uses evidences based on the learners activities and maps these learners to their competencies  We propose a model that explores the activity data of the learners generated while achieving the competency The data collected implicitly in a continuous fashion is modeled to map the learners to their competencies  Evidences can be considered analogous to an observation that an individual makes in the classroom through interaction or silent observation of reactions which contains significant insights An offline system like a classroom is unable to gather data for each individual during the learning process but when learning happens in a Technology Assisted Learning EnvironmentTALE data is continuously captured by the platform Technology augmentation can happen in various ways  like sensors and RFID tags to record attendance recording and analysis of students classroom activities like questions discussions etc We want to focus on the evidences so that we dont have to rely only on the formal assessments to determine the competency of the learner   A competency model based on evidences can also be used to identify anomalous cases where the outcomes state that the individual has the competency while the evidence indicates a lack of competency or viceversa A teacher can only analyze such cases and address them appropriately The model based on evidences acts as a way of aiding the teacher in teaching and evaluating process  It can also help the students identify areas they are weak in and help them acquire the competency  Newer learning domains like training drivers to learn a particular language that would improve their communication skills may not have standardized competency models to aid in pedagogy and assessments In such cases models based on evidences would help to map the learners to their competencies   The main challenge is to create an evidence model for collecting data and to argue for the completeness of the evidence Learning may happen outside of the evidence gathering and different kinds of learning activities may require different kinds of evidences to reason about them   In order to address the above challenge we adopt a leastbiased model for evidence modeling and treat each form of evidence as equally important in the input feature vector All forms of evidences collected are then given as input to a machine learning algorithm to find the best possible indicators for the outcomes based on assessment scores   Experiments and Initial Results  The activity data used to build models is collected from a large open online learning platform implemented across several schools in the US The platform has aggregated open learning resources for various courses provided by content creators curators and instructors The learning resource can be a document video audio puzzles or any content used to obtain the competency Learners enroll to various courses   A course is organized into several competencies where a competency is seen as the basic unit of learning Each competency may have several learning resources mapped to it Students consume learning resources and whenever they are ready give assessments to earn a score Instructors evaluate the assessments and provide their feedback in the form of scores The scores indicate the status of the learner with respect to the competency The learning resources are mapped to competencies for various courses like Maths Science English and Social Science in the K12 curriculum  Each competency also has a signature assessment that the learner has to take in order to earn a status for that competency   The activity data collected for a learner competency pair is divided into collections and assessments Collections are resources used during the learning process while assessments act as indicators of learning In the platform a learner is said to have achieved a completed status for a competency if one gets more than 80 in the respective assessments The platform does not award a fail status to the learner The learner keeps attempting assessments multiple times until the learner gets 80 or more The status is then set to completed else the status is marked as inprogress The data used to build Evidence based competency model contains learners who have completed status as well as inprogress status with respect to a particular competency  Whenever a learner consumes a resource to learn a concept an event is logged in the system with the details of the resource time of the event learner details and type of eventstarted consuming the resource The same process is repeated when the learner finishes consuming the resource with a stop event The events are captured for all the courses Individuals consume various resources mapped to the same competency and at the end give assessments to get a particular status for that competency    Using these activity data we have built a model to determine the outcome of the competency based on the evidences  Experiment 1  We built a Support Vector Machine SVM classifier to test a hypothesis Our hypothesis is as follows Ht The time spent on learning resources the total number of resources consumed by learners can predict the outcome for the underlying competency  The features identified are total time spent on resources average time spent on resources and number of resources used for acquiring the corresponding competency The users were given a completed or inprogress status based on their assessment scores That serves as the ground truth for our model   The dataset has 28000 usercompetency pairs with their corresponding evidences This data was divided into 8020 split randomly for training and testing the SVM model respectively We built a single SVM model for all learners and their competencies in all courses We classified the data using linear polynomial sigmoid and radial basis kernel function The data was not linearly separable The radial basis kernel function was found to be the best kernel function to classify the data in terms of Accuracy Precision and F1 score as shown in Table 1   Kernel functions Accuracy Precision F1 score Radial basis 8243 6879 5113 Linear 7839 5365 4030 Polynomial 7883 5625 3791 Sigmoid 6884 3068 3037  Table 1 Performance metrics comparing the kernels  The accuracy measure of the SVM model using the radial basis function states that using total time spent average time spent on the resources and number of resources we can significantly predict the outcome of the underlying competency The outcomes are determined by the scores of the assessments which was not used as feature to build the models This shows that evidence can be used as an alternate way to model the outcome of the underlying competency   The same model was made to classify all the competencies of a random individual learner the accuracy of the average SVM model varied between 30 to 100 for different learners  The average learner model computed above was not effective in determining the outcome of a competency of an individual learner This indicates that we cannot use one single aggregated model on all individuals There is a need to build individual models to understand the evidences better and improve the way of mapping learners to their competency  Models must consider personalization ie we need to build individual models for each users and aggregate the models based on common properties  To do this we require significant amount of data for each learner So instead of aggregating time spent on resources at a competency level as total time we looked at time spent for each resource and aggregated the data separately for each learner in the next experiment   Experiment 2 In this experiment we used data from each activity event and modeled the time spent on each resource mapped to a particular competency Using the start and the stop time we computed the time spent on each resource and we also observed that some individuals consumed the same resource again Based on this we formed our second hypothesis Hu There is a large variance among the individual models built for each learner based on their learning activity data  We wanted to observe the consumption of resources and the time spent on those resources could predict the outcome of the corresponding competency for that individual learner We also wanted to observe if the models built for each individual learner had common properties and could be merged to a single model or there is a large variance among them   The amount of time spent on each resource is stored as a vector for each learnercompetency pair the length of the vector is the number of resources and the order of the vector tells the order in which the resources were consumed The length of the vector varied for each learnercompetency pair To build individual learner models and compare them we require equal number of features for all pairs of learner competency   To achieve this we transformed the time spent on resource vector to a matrix in the following way We computed the range of total time spent on resourcecompetency pair by all learners This value varied from 10 seconds to 46000 seconds After observing this distribution we decided to have a time frequency of 100 seconds and computed the cumulative sum of resources consumed by the learner at each frequency ie 100th second 200th second etc  We populated 460 time frequency columns We arrived at this time frequency value of 100 seconds by dividing the maximum total time and time frequency   For example if the learner has consumed 2 resources for a competency code 3 spending 90 seconds on first resource and 170 seconds on second resource then the column 0time100 will have the value 1 column 1time200 will have the value 1 as the learner has not finished consuming the second resource by 200 seconds column 2time300 has the value 2 and rest of the columns till column 459time460 will have the value 2 indicating that the user consumed maximum 2 resources Fig1 shows the subset of the data passed to the model for a single user where code refers to competency code and rest of the columns are the evidences for that competency code for a single learner The row containing code value 3 shows the corresponding result of example mentioned above The features also includes normalized totaltime spent on resources and normalized averagetime spent on those resources for a particular competency learner pair   Figure 1 Data passed to the classifier with cumulative frequency of resources with respect to time total time and average time as features  SVM model with linear kernel was used to classify the data Learners who had more than 30 competencies in any course and with any status completed or inprogress were selected for analysis There were 42 learners who satisfied that criteria and individual SVM models were built for those learners Individual learners data was divided into 8020 split randomly for training and testing respectively The models for each learner gave an accuracy between 80 to 100 The accuracy was calculated from the confusion matrix generated for each individual model according to the formula mentioned in Each model generated a coefficient vector of length 462 We also built a single model comprising the activity data all these 42 learners and called this the averagemodel as detailed in Experiment 1 for comparison   All the 43 vectors of coefficients for the 43 models populated were clustered using the Knearest neighbor clustering algorithm to observe any similarity among these models The Euclidean distance between the data points models was used as a measure to cluster the models To find the appropriate number of clusters Elbow method was used to find the optimal number of clusters The summation of distance between that specific cluster against the cluster centroid was computed and plotted for various number of clusters between 1 and 43 where 43 was the total number of models  The graph in Fig 2 shows the variance in the sum of the squared distance between clusters against the number of clusters  Figure 2 Elbow method showing the variance of number of clusters and their distances  Using the variance in the Elbow method we found the optimal number of clusters to be 6 This says that the 43 models including the average model can be clustered into 6 clusters and there are some common properties among these models   Fig3 shows the distribution of 43 different models including the average model into 6 different clusters The blue point refers to individual learner models and the red point indicates the average learner model   Figure 3 Distribution of individual learners models and average model into 6 clusters  We see that the individual models dont cluster with the average model For this dataset the average model does not represent any individual learner This validates our hypothesis that there is large variance among the individual models built for each learner based on their learning activity data and hence validates The Myth of the Average It also shows that we need not build one model for each individual which would make it extremely difficult to map new learners to their competencies We find that there are some common properties among the learners which can be used to find the best model for each learner We need to determine those common properties in the future and provide better learning experiences to each individual learner  Conclusion  We proposed an Evidence Based Competency Model that uses data generated during the learning process by learners to find the outcome of the competency The experiments shows that having a single average model can be tuned to get a higher accuracy but does not represent any individual This presses the need to consider individual learners variance and not rely on statistical averages to map learners to their competencies This also says that we cannot provide each learner the same learning experience and cannot validate their underlying competencies through uniform evaluation mechanisms like outcomes This research shows initial results towards that direction  5 \n"
     ]
    }
   ],
   "source": [
    "print(new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87476481",
   "metadata": {},
   "source": [
    "### Setting min and max length of phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa4d0d2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = Rake(min_length=1, max_length=6)\n",
    "r.extract_keywords_from_text(new_data)\n",
    "out = r.get_ranked_phrases()\n",
    "out_with_scores = r.get_ranked_phrases_with_scores()\n",
    "# print(out_with_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8be117",
   "metadata": {},
   "source": [
    "#### Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "367d0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "result = []\n",
    "# for item in out_with_scores:\n",
    "for item in out:\n",
    "    if item not in seen:\n",
    "        seen.add(item)\n",
    "        result.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e44d7375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['emerging approaches towards standardizing pedagogy models', 'learner keeps attempting assessments multiple times', 'competency models must consider personalization ie', 'row containing code value 3 shows', 'various courses like maths science english', 'competency code 3 spending 90 seconds', 'clusters figure 2 elbow method showing', 'features also includes normalized totaltime spent', 'average abilities however recent advances', 'average learner model figure 3 distribution', 'research proposes evidence based competency model', 'learning activities may require different kinds', 'courses individuals consume various resources mapped', 'research shows initial results towards', 'disparate individuals behave learn develop', 'learning cultures like constructivist education', 'outcome based education obe model focuses', 'uniform evaluation mechanisms like outcomes', 'learning activities like watching video', 'cannot use one single aggregated model', 'hypothetical average individual however research', 'technology assisted learning environmenttale data', 'study also shows significant variability', 'data using linear polynomial sigmoid', 'populated 460 time frequency columns', 'support vector machine svm classifier', 'evidence learning may happen outside', 'group averages designing models based', 'determining learners latent competency levels', 'various ways like sensors', 'document video audio puzzles', 'radial basis function states', 'knearest neighbor clustering algorithm', 'providing individualized learning experiences', 'formal processes mass production', 'columns till column 459time460', 'radial basis kernel function', 'use machine learning techniques', 'different interests disposition contexts', '200 seconds column 2time300', 'learn different topics one', 'assessment practices without regard', 'students consume learning resources', 'next experiment experiment 2', 'value 1 column 1time200', 'aggregated open learning resources', 'one single average model', 'provide better learning experiences', 'variability among individual learners', '43 different models including', 'consider individual learners variance', 'formal educational practices based', 'produce specific measurable outcomes', 'average svm model varied', 'several learning resources mapped', 'evidence based competency model', 'learning process individual models', 'machine learning algorithm', 'various courses provided', 'resources average time spent', 'normalized averagetime spent', 'build individual learner models', 'individual models dont cluster', 'communication skills may', 'fig 2 shows', 'hypothetical average student', 'offline system like', 'require significant amount', 'contains significant insights', 'best kernel function', 'value 2 indicating', 'learners individual learners data', 'students identify areas', 'clusters elbow method', 'single svm model', 'average learner model computed', 'build one model', 'implicit continuous basis', 'models fig3 shows', 'learning practices based', 'features svm model', '43 models populated', 'behaviorist learning practices', 'uniform model based', 'svm model using', 'record attendance recording', 'question existing methods', 'problem solving upholding', 'correlate different types', '8020 split randomly', 'individual svm models', 'single average model', 'svm model respectively', '43 models including', 'blue point refers', 'contrast evidence pertains', 'require equal number', 'build separate models', 'provide pedagogical solutions', 'platform technology augmentation', 'content creators curators', 'time frequency value', 'create empowered individuals', 'single model comprising', 'identify anomalous cases', 'individual model according', 'aggregating time spent', 'large variance among', 'data points models', 'cases models based', 'learning process referred', 'ready give assessments', 'end give assessments', 'build individual models', 'standardized competency models', 'maximum total time', 'specific learning activity', 'competency model based', 'red point indicates', 'dimensions rapidly diminishes', '28000 usercompetency pairs', 'learning patterns among', 'uses data generated', 'informal data generated', 'uses evidences based', 'model evidence modeling', 'average model validating', 'total time spent', 'formal testing environments', 'activity data rather', 'term evidence refers', 'common properties among', 'learning activity data', 'data collected implicitly', 'visible outcomes discounts', 'follows outcomes refer', 'random individual learner', 'individual learner conclusion', 'individual learners models', 'consumed 2 resources', 'found vast discrepancies', 'best possible indicators', 'time total time', 'particular competency using', 'classroom imparts skills', '6 different clusters', 'individual learner models', 'competency learners enroll', 'assessment data collected', 'score instructors evaluate', 'different kinds', 'confusion matrix generated', 'learner finishes consuming', 'various courses', 'elbow method', 'activity data collected', 'assessment scores experiments', 'learners hence validating', 'individual models built', 'evidences experiment 1', 'map new learners', 'initial results', 'second hypothesis hu', 'particular competency whenever', 'linear kernel', 'learner competency pair', 'value 2', 'inprogress status based', 'acquire relevant competencies', 'one gets', 'learner gets 80', 'activity data used', 'competency may', 'code refers', 'input feature vector', 'group averages', 'educational practices', 'evidences would help', 'event learner details', 'social science', 'education comprising', 'learning activities', 'resources could predict', 'models based', 'competency code', 'experiments shows', 'also shows', 'single model', 'value varied', 'model based', 'different learners', 'specific activities', 'average model', 'build models', 'value 1', 'column 0time100', 'cannot provide', 'best model', 'various number', 'individuals consumed', 'evidence model', 'average time', 'learning data', 'time frequency', 'competency based', 'time spent', 'outcomes based', 'competency cannot', 'learners activities', 'single learner', 'model generated', 'measurable outcomes', 'experiment 1', 'learning process', 'learner based', 'learning outcomes', 'term evidence', 'evidence modeling', 'single user', 'learning resources', 'total time', 'competency using', 'specific cluster', 'similarity among', 'interests standardization', 'features identified', 'cannot validate', 'formal assessments', 'learning happens', 'learning experience', 'vector varied', 'data generated', '46000 seconds', '170 seconds', '10 seconds', 'evidences better', 'individual learners', 'uniform teaching', 'distances using', 'activity data', 'average discussed', 'signature assessment', 'cumulative frequency', 'common properties', 'competency also', 'learners generated', 'statistical averages', 'resource mapped', 'leastbiased model', 'data collected', 'visible outcomes', 'individual learner', 'data learners', 'evidence indicates', 'evidence gathering', 'assessment scores', 'individual traits', 'individual makes', 'stop time', 'models built', 'testing respectively', 'several competencies', 'particular language', 'follows ht', 'f1 score', 'correlate characteristics', 'continuous fashion', 'also observed', 'also help', 'also considered', 'learning resource', 'activity event', 'learner competency', 'gather data', 'data separately', 'data passed', 'collecting data', '100 seconds', 'classroom model', 'would make', 'finished consuming', 'eventstarted consuming', '43 vectors', 'underlying competency', 'corresponding competency', 'competency level', 'mapping learners', '42 learners', 'vector tells', 'resourcecompetency pair', 'learnercompetency pair', 'evaluating process', 'coefficient vector', 'cluster centroid', 'particular status', 'evidences collected', 'resources consumed', 'clustered using', 'also built', 'respective assessments', 'assessments act', 'total number', 'learner pertinent', 'learner pertaining', 'learner gave', 'learner consumes', '6 clusters', 'resource time', 'map learners', 'usually ineffective', 'target beneficiary', 'tacit elements', 'strongly rooted', 'stop event', 'standardization curricula', 'rfid tags', 'linearly separable', 'k12 curriculum', 'holistic nature', 'hence validates', 'ground truth', 'generic approach', 'fully cognizant', 'formula mentioned', 'extremely difficult', 'explicitly prepared', 'equally important', 'direction 5', 'current implementation', 'corresponding result', 'considered analogous', 'concern increase', 'basic unit', 'also wanted', 'also show', 'also says', 'content used', 'used data', 'data used', 'evidences acts', 'corresponding evidences', 'would improve', 'second resource', 'scores indicate', 'completed else', 'learners ability', 'significantly predict', 'resource vector', 'outcomes state', 'fail status', 'squared distance', 'euclidean distance', 'use', 'aggregated', 'assessments collections', 'resources used', 'courses', 'optimal number', 'appropriate number', 'status completed', 'completed status', 'underlying competencies', 'silent observation', 'popularly called', 'pedagogy', 'obe', 'main challenge', 'higher accuracy', 'following way', 'first resource', 'example mentioned', 'education', 'dont focus', 'cumulative sum', 'continuously captured', 'alternate way', 'accuracy precision', 'inprogress status', 'individuals', 'shows', 'competencies evidences', 'columns', 'length 462', 'observe similarity', 'mapped', 'based', 'happen', 'features', 'classifier', 'activities', 'accuracy measure', '30 competencies', 'learning', 'models', 'average', 'experiment', 'learn', 'model', 'build', 'distribution', 'evidence', 'individual', 'provide', 'also', 'activity', 'data', '1', 'validating', 'dont', 'determining', 'collected', 'cases', '43', 'variance', 'competency', 'learners', 'vector', 'process', 'cluster', 'outcomes', 'resources', 'assessments', 'learner', 'clusters', 'whenever', 'testing', 'system', 'patterns', 'pairs', 'matrix', 'instructors', 'input', 'indicators', 'indicates', 'help', 'feature', 'event', 'dimensions', 'details', 'create', 'could', 'consumed', 'amount', 'acquire', '6', 'evidences', 'scores', 'completed', 'predict', 'map', 'inprogress', 'status', 'found', 'distance', 'built', '80', 'outcome', 'computed', 'classroom', 'number', 'way', 'wanted', 'validates', 'teaching', 'sum', 'state', 'show', 'says', 'resource', 'platform', 'observation', 'measure', 'improve', 'hypothesis', 'example', 'competencies', 'collections', 'challenge', 'captured', 'called', 'accuracy', 'ability', '30', '100', 'used', 'length', 'clustered', 'observe', 'focus', 'zero', 'work', 'well', 'weak', 'want', 'viceversa', 'users', 'us', 'understand', 'unable', 'type', 'tuned', 'treat', 'transformed', 'training', 'test', 'terms', 'teacher', 'take', 'summation', 'subset', 'styles', 'stored', 'start', 'shown', 'set', 'serves', 'selected', 'seen', 'see', 'scale', 'satisfied', 'said', 'rest', 'respect', 'represent', 'repeated', 'rely', 'reason', 'reactions', 'range', 'proposed', 'propose', 'probability', 'presses', 'possessing', 'plotted', 'part', 'organized', 'order', 'obtain', 'observing', 'objective', 'needs', 'need', 'myth', 'modeled', 'merged', 'mechanism', 'marked', 'maps', 'made', 'looked', 'logged', 'lead', 'lack', 'knowledge', 'kind', 'kernels', 'issues', 'interaction', 'instead', 'individualization', 'individuality', 'incompatible', 'implication', 'graph', 'given', 'get', 'future', 'forms', 'formed', 'form', 'follow', 'finding', 'find', 'feedback', 'factory', 'explores', 'events', 'emphasis', 'effective', 'earn', 'dividing', 'divided', 'determined', 'determine', 'detailed', 'detail', 'designed', 'decided', 'dataset', 'criteria', 'creation', 'course', 'contrasted', 'consumption', 'concept', 'completeness', 'comparison', 'compare', 'collect', 'coefficients', 'classify', 'classified', 'capture', 'capable', 'calculated', 'award', 'averagemodel', 'assessed', 'arrived', 'argue', 'area', 'appropriately', 'apply', 'analyze', 'analysis', 'aiding', 'aid', 'aggregate', 'adopting', 'adopt', 'addressing', 'addressed', 'address', 'acquiring', 'achieving', 'achieved', 'achieve', 'able']\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9eb061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "34df51a6e8bd86dc0812573b711439a9ba498e102ba9a2953849f0f753f12b8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
